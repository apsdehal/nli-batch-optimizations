{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from data_iterator import TextIterator\n",
    "from torch.autograd import Variable\n",
    "import pickle as pkl\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_words = 100140\n",
    "batch_size=32\n",
    "valid_batch_size=32\n",
    "dim_word = 300\n",
    "char_nout = 100\n",
    "dim_char_emb = 15\n",
    "learning_rate = 0.01\n",
    "dim_hidden = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.5343  0.3788 -0.4253 -1.2913\n",
      "-0.7570 -0.5743 -0.0250 -0.4239\n",
      " 0.3467  0.0604 -1.5628  0.7043\n",
      "-0.5295  0.9500 -0.2956  0.0069\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.3788\n",
       " -0.0250\n",
       "  0.7043\n",
       "  0.9500\n",
       " [torch.FloatTensor of size 4], Variable containing:\n",
       "  1\n",
       "  2\n",
       "  3\n",
       "  1\n",
       " [torch.LongTensor of size 4])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = Variable(torch.Tensor([[1,2,3],[4,5,6]]))\n",
    "\n",
    "a = Variable(torch.randn(4, 4))\n",
    "print(a)\n",
    "torch.max(a,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str2list(s):\n",
    "    alphabet = \" abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    l = len(s)\n",
    "    ans = []\n",
    "    for i in range(0, l):\n",
    "        a = alphabet.find(s[i])\n",
    "        if a >= 0:\n",
    "            ans.append(a)\n",
    "        else:\n",
    "            ans.append(0)\n",
    "            #print(s[i])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(seqs_x, seqs_y, labels, worddicts_r, maxlen=None):\n",
    "    # x: a list of sentences\n",
    "    lengths_x = [len(s) for s in seqs_x]\n",
    "    lengths_y = [len(s) for s in seqs_y]\n",
    "\n",
    "    if maxlen is not None:\n",
    "        new_seqs_x = []\n",
    "        new_seqs_y = []\n",
    "        new_lengths_x = []\n",
    "        new_lengths_y = []\n",
    "        new_labels = []\n",
    "        for l_x, s_x, l_y, s_y, l in zip(lengths_x, seqs_x, lengths_y, seqs_y, labels):\n",
    "            if l_x < maxlen and l_y < maxlen:\n",
    "                new_seqs_x.append(s_x)\n",
    "                new_lengths_x.append(l_x)\n",
    "                new_seqs_y.append(s_y)\n",
    "                new_lengths_y.append(l_y)\n",
    "                new_labels.append(l)\n",
    "        lengths_x = new_lengths_x\n",
    "        seqs_x = new_seqs_x\n",
    "        lengths_y = new_lengths_y\n",
    "        seqs_y = new_seqs_y\n",
    "        labels = new_labels\n",
    "\n",
    "        if len(lengths_x) < 1 or len(lengths_y) < 1:\n",
    "            return None\n",
    "\n",
    "    max_char_len_x = 0\n",
    "    max_char_len_y = 0\n",
    "    seqs_x_char = []\n",
    "    l_seqs_x_char = []\n",
    "    seqs_y_char = []\n",
    "    l_seqs_y_char = []\n",
    "\n",
    "    for idx, [s_x, s_y, s_l] in enumerate(zip(seqs_x, seqs_y, labels)):\n",
    "        temp_seqs_x_char = []\n",
    "        temp_l_seqs_x_char = []\n",
    "        temp_seqs_y_char = []\n",
    "        temp_l_seqs_y_char = []\n",
    "        for w_x in s_x:\n",
    "            word = worddicts_r[w_x]\n",
    "            word_list = str2list(word)\n",
    "            l_word_list = len(word_list)\n",
    "            temp_seqs_x_char.append(word_list)\n",
    "            temp_l_seqs_x_char.append(l_word_list)\n",
    "            if l_word_list >= max_char_len_x:\n",
    "                max_char_len_x = l_word_list\n",
    "        for w_y in s_y:\n",
    "            word = worddicts_r[w_y]\n",
    "            word_list = str2list(word)\n",
    "            l_word_list = len(word_list)\n",
    "            temp_seqs_y_char.append(word_list)\n",
    "            temp_l_seqs_y_char.append(l_word_list)\n",
    "            if l_word_list >= max_char_len_y:\n",
    "                max_char_len_y = l_word_list\n",
    "\n",
    "        seqs_x_char.append(temp_seqs_x_char)\n",
    "        l_seqs_x_char.append(temp_l_seqs_x_char)\n",
    "        seqs_y_char.append(temp_seqs_y_char)\n",
    "        l_seqs_y_char.append(temp_l_seqs_y_char)\n",
    "\n",
    "    n_samples = len(seqs_x)\n",
    "    maxlen_x = numpy.max(lengths_x)\n",
    "    maxlen_y = numpy.max(lengths_y)\n",
    "\n",
    "    x = numpy.zeros((maxlen_x, n_samples)).astype('int64')\n",
    "    y = numpy.zeros((maxlen_y, n_samples)).astype('int64')\n",
    "    x_mask = numpy.zeros((maxlen_x, n_samples)).astype('float32')\n",
    "    y_mask = numpy.zeros((maxlen_y, n_samples)).astype('float32')\n",
    "    l = numpy.zeros((n_samples,)).astype('int64')\n",
    "    char_x = numpy.zeros((maxlen_x, n_samples, max_char_len_x)).astype('int64')\n",
    "    char_x_mask = numpy.zeros((maxlen_x, n_samples, max_char_len_x)).astype('float32')\n",
    "    char_y = numpy.zeros((maxlen_y, n_samples, max_char_len_y)).astype('int64')\n",
    "    char_y_mask = numpy.zeros((maxlen_y, n_samples, max_char_len_y)).astype('float32')\n",
    "\n",
    "    for idx, [s_x, s_y, ll] in enumerate(zip(seqs_x, seqs_y, labels)):\n",
    "        x[:lengths_x[idx], idx] = s_x\n",
    "        x_mask[:lengths_x[idx], idx] = 1.\n",
    "        y[:lengths_y[idx], idx] = s_y\n",
    "        y_mask[:lengths_y[idx], idx] = 1.\n",
    "        l[idx] = ll\n",
    "\n",
    "        for j in range(0, lengths_x[idx]):\n",
    "            char_x[j, idx, :l_seqs_x_char[idx][j]] = seqs_x_char[idx][j]\n",
    "            char_x_mask[j, idx, :l_seqs_x_char[idx][j]] = 1.\n",
    "        for j in range(0, lengths_y[idx]):\n",
    "            char_y[j, idx, :l_seqs_y_char[idx][j]] = seqs_y_char[idx][j]\n",
    "            char_y_mask[j, idx, :l_seqs_y_char[idx][j]] = 1.\n",
    "\n",
    "    return x, x_mask, char_x, char_x_mask, y, y_mask, char_y, char_y_mask, l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NLI(nn.Module):\n",
    "    def __init__(self, dim_word, char_nout, dim_char_emb, word_embeddings_file, worddict, num_words, dim_hidden):\n",
    "        super(NLI, self).__init__()\n",
    "        self.dim_word = dim_word\n",
    "        self.char_nout = char_nout\n",
    "        self.dim_char_emb = dim_char_emb \n",
    "        self.char_k_cols = dim_char_emb\n",
    "        self.char_k_rows=[1,3,5]\n",
    "        self.hidden_size = dim_hidden\n",
    "        self.word_embeddings = self.create_word_embeddings(word_embeddings_file, worddict, num_words, dim_word)\n",
    "        \n",
    "        dim_emb = dim_word + 3*char_nout\n",
    "        self.alphabet = \" abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "        self.filter1 = None\n",
    "        self.filter2 = None\n",
    "        self.filter3 = None\n",
    "        \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(dim_emb, dim_hidden, 1, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(dim_emb+2*dim_hidden, dim_hidden, 1, bidirectional=True)\n",
    "        self.lstm3 = nn.LSTM(dim_emb+2*dim_hidden, dim_hidden, 1, bidirectional=True)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def create_word_embeddings(self, file_name, worddicts, num_words, dim_word):\n",
    "        word_embeddings = Variable(torch.from_numpy(self.norm_weight(num_words, dim_word)))\n",
    "        with open(file_name, 'r') as f:\n",
    "            for line in f:\n",
    "                tmp = line.split()\n",
    "                word = tmp[0]\n",
    "                vector = tmp[1:]\n",
    "                len_vec = len(vector)\n",
    "                \n",
    "                if(len_vec>300):\n",
    "                    diff = len_vec-300\n",
    "                    word = word.join(vector[:diff])\n",
    "                    vector = vector[diff:]\n",
    "                    \n",
    "                    \n",
    "                if word in worddicts and worddicts[word] < num_words:\n",
    "                    vector = [float(x) for x in vector]\n",
    "                    word_embeddings[worddicts[word], :] = torch.FloatTensor(vector[0:300])\n",
    "                    \n",
    "        return word_embeddings\n",
    "        \n",
    "    def forward(self, premise, char_premise, premise_mask, char_premise_mask, hypothesis, char_hypothesis, hypothesis_mask, char_hypothesis_mask,l):\n",
    "        #premise = number of words * number of samples. Also hypothesis = number of words * number of samples\n",
    "        n_timesteps_premise = premise.size(0)\n",
    "        n_timesteps_hypothesis = hypothesis.size(0)\n",
    "        n_samples = premise.size(1)\n",
    "        \n",
    "        premise_char_vector = self.compute_character_embeddings(char_premise, n_timesteps_premise, n_samples, char_premise_mask)\n",
    "        hypothesis_char_vector = self.compute_character_embeddings(char_hypothesis, n_timesteps_hypothesis, n_samples, char_hypothesis_mask)\n",
    "\n",
    "        premise_word_emb = self.word_embeddings[premise.view(-1)].view(n_timesteps_premise, n_samples, self.dim_word)\n",
    "        hypothesis_word_emb = self.word_embeddings[hypothesis.view(-1)].view(n_timesteps_hypothesis, n_samples, self.dim_word)\n",
    "        \n",
    "        hypothesis_emb = torch.cat([hypothesis_word_emb, hypothesis_char_vector], 2)\n",
    "        premise_emb = torch.cat([premise_word_emb, premise_char_vector], 2)\n",
    "        \n",
    "        premise_h1_0 = Variable(torch.zeros(2, n_samples, self.hidden_size)) # 2 for bidirection \n",
    "        premise_c1_0 = Variable(torch.zeros(2, n_samples, self.hidden_size))\n",
    "        \n",
    "        premise_h2_0 = Variable(torch.zeros(2, n_samples, self.hidden_size)) # 2 for bidirection \n",
    "        premise_c2_0 = Variable(torch.zeros(2, n_samples, self.hidden_size))\n",
    "        \n",
    "        premise_h3_0 = Variable(torch.zeros(2, n_samples, self.hidden_size)) # 2 for bidirection \n",
    "        premise_c3_0 = Variable(torch.zeros(2, n_samples, self.hidden_size))\n",
    "\n",
    "        hypothesis_h1_0 = Variable(torch.zeros(2, n_samples, self.hidden_size)) # 2 for bidirection \n",
    "        hypothesis_c1_0 = Variable(torch.zeros(2, n_samples, self.hidden_size))\n",
    "        \n",
    "        hypothesis_h2_0 = Variable(torch.zeros(2, n_samples, self.hidden_size)) # 2 for bidirection \n",
    "        hypothesis_c2_0 = Variable(torch.zeros(2, n_samples, self.hidden_size))\n",
    "        \n",
    "        hypothesis_h3_0 = Variable(torch.zeros(2, n_samples, self.hidden_size)) # 2 for bidirection \n",
    "        hypothesis_c3_0 = Variable(torch.zeros(2, n_samples, self.hidden_size))\n",
    "        \n",
    "        hypothesis_layer1 = self.lstm1(hypothesis_emb, (hypothesis_h1_0, hypothesis_c1_0))\n",
    "        hypothesis_layer2 = self.lstm2(torch.cat([hypothesis_emb,hypothesis_layer1[0]],2), (hypothesis_h2_0, hypothesis_c2_0))\n",
    "        hypothesis_layer3 = self.lstm3(torch.cat([hypothesis_emb,hypothesis_layer2[0]],2), (hypothesis_h3_0, hypothesis_c3_0))\n",
    "\n",
    "        premise_layer1 = self.lstm1(premise_emb, (premise_h1_0, premise_c1_0))\n",
    "        premise_layer2 = self.lstm2(torch.cat([premise_emb,premise_layer1[0]],2), (premise_h2_0, premise_c2_0))\n",
    "        premise_layer3 = self.lstm3(torch.cat([premise_emb,premise_layer2[0]],2), (premise_h3_0, premise_c3_0))\n",
    "        \n",
    "        return premise_layer3\n",
    "        \n",
    "    \n",
    "        \n",
    "    def compute_character_embeddings(self, chars_word, n_timesteps, num_samples, char_mask):\n",
    "        emb_char = self.Charemb[chars_word.view(-1)].view(n_timesteps, num_samples, chars_word.size(2), self.dim_char_emb)\n",
    "        emb_char = emb_char * char_mask[:, :, :, None]\n",
    "        emb_char_inp = emb_char.view(n_timesteps * num_samples, 1, chars_word.size(2), self.dim_char_emb)\n",
    "\n",
    "        char_level_emb1 = self.apply_filter_and_get_char_embedding(self.filter1, emb_char_inp, num_samples, n_timesteps)\n",
    "        char_level_emb2 = self.apply_filter_and_get_char_embedding(self.filter2, emb_char_inp, num_samples, n_timesteps)\n",
    "        char_level_emb3 = self.apply_filter_and_get_char_embedding(self.filter3, emb_char_inp, num_samples, n_timesteps)\n",
    "        \n",
    "        emb_chars = [char_level_emb1, char_level_emb2, char_level_emb3]\n",
    "        emb_char = torch.cat(emb_chars,2)\n",
    "        return emb_char\n",
    "    \n",
    "    def apply_filter_and_get_char_embedding(self, filter_type, emb_char_inp, n_samples, n_timesteps):\n",
    "        emb_char = F.conv2d(emb_char_inp, filter_type)\n",
    "        emb_char = F.relu(emb_char)\n",
    "        emb_char = emb_char.view(n_timesteps * n_samples, self.char_nout, emb_char.size(2))\n",
    "        emb_char = emb_char.max(2)[0]\n",
    "        emb_char = emb_char.view(n_timesteps, n_samples, self.char_nout)\n",
    "        return emb_char\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.Charemb = Variable(torch.from_numpy(self.norm_weight(len(self.alphabet) + 1, self.dim_char_emb)), requires_grad = True)\n",
    "        self.filter1 = self.getFilter(self.char_k_rows[0])\n",
    "        self.filter2 = self.getFilter(self.char_k_rows[1])\n",
    "        self.filter3 = self.getFilter(self.char_k_rows[2])\n",
    "        \n",
    "    \n",
    "       \n",
    "    def getFilter(self, char_k_row):\n",
    "        w_shp = (self.char_nout, 1, char_k_row, self.char_k_cols)\n",
    "        w_bound = numpy.sqrt(3 * char_k_row * self.char_k_cols)\n",
    "        return Variable(torch.from_numpy(numpy.random.uniform(low=-1.0/w_bound, high=1.0/w_bound, size=w_shp).astype('float32')))\n",
    "\n",
    "\n",
    "\n",
    "    def norm_weight(self, nin, nout=None, scale=0.01, ortho=True):\n",
    "        \"\"\"\n",
    "        Random weights drawn from a Gaussian\n",
    "        \"\"\"\n",
    "        if nout is None:\n",
    "            nout = nin\n",
    "        if nout == nin and ortho:\n",
    "            W = self.ortho_weight(nin)\n",
    "        else:\n",
    "            W = scale * numpy.random.randn(nin, nout)\n",
    "        return W.astype('float32')\n",
    "\n",
    "    def ortho_weight(self, ndim):\n",
    "        \"\"\"\n",
    "        Random orthogonal weights\n",
    "        Used by norm_weights(below), in which case, we\n",
    "        are ensuring that the rows are orthogonal\n",
    "        (i.e W = U \\Sigma V, U has the same\n",
    "        # of rows, V has the same # of cols)\n",
    "        \"\"\"\n",
    "        W = numpy.random.randn(ndim, ndim)\n",
    "        u, s, v = numpy.linalg.svd(W)\n",
    "        return u.astype('float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TextIterator('word_sequence/premise_multinli_1.0_train.txt',\n",
    "                     'word_sequence/hypothesis_multinli_1.0_train.txt',\n",
    "                     'word_sequence/label_multinli_1.0_train.txt',\n",
    "                     'word_sequence/vocab_cased.pkl',\n",
    "                      n_words=num_words,\n",
    "                      batch_size=batch_size)\n",
    "valid = TextIterator('word_sequence/premise_multinli_1.0_dev_matched.txt',\n",
    "                     'word_sequence/hypothesis_multinli_1.0_dev_matched.txt',\n",
    "                     'word_sequence/label_multinli_1.0_dev_matched.txt',\n",
    "                     'word_sequence/vocab_cased.pkl',\n",
    "                      n_words=num_words,\n",
    "                      batch_size=valid_batch_size,\n",
    "                      shuffle=False)\n",
    "\n",
    "test = TextIterator('word_sequence/premise_multinli_1.0_dev_mismatched.txt',\n",
    "                    'word_sequence/hypothesis_multinli_1.0_dev_mismatched.txt',\n",
    "                    'word_sequence/label_multinli_1.0_dev_mismatched.txt',\n",
    "                    'word_sequence/vocab_cased.pkl',\n",
    "                    n_words=num_words,\n",
    "                    batch_size=valid_batch_size,\n",
    "                    shuffle=False)\n",
    "\n",
    "with open('word_sequence/vocab_cased.pkl', 'rb') as f:\n",
    "    worddicts = pkl.load(f)\n",
    "worddicts_r = dict()\n",
    "\n",
    "for kk, vv in worddicts.items():\n",
    "    worddicts_r[vv] = kk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = NLI(dim_word, char_nout, dim_char_emb, 'data/glove.840B.300d.txt', worddicts, num_words, dim_hidden)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "print('training')\n",
    "\n",
    "for x1, x2, y in train:\n",
    "    premise, premise_mask, char_premise, char_premise_mask, hypothesis, hypothesis_mask, char_hypothesis, char_hypothesis_mask, l = prepare_data(x1,x2,y,worddicts_r)\n",
    "    premise = Variable(torch.from_numpy(premise))\n",
    "    premise_mask = Variable(torch.from_numpy(premise_mask))\n",
    "    char_premise = Variable(torch.from_numpy(char_premise))\n",
    "    char_premise_mask = Variable(torch.from_numpy(char_premise_mask))\n",
    "    hypothesis = Variable(torch.from_numpy(hypothesis))\n",
    "    hypothesis_mask = Variable(torch.from_numpy(hypothesis_mask))\n",
    "    char_hypothesis = Variable(torch.from_numpy(char_hypothesis))\n",
    "    char_hypothesis_mask = Variable(torch.from_numpy(char_hypothesis_mask))\n",
    "    l = Variable(torch.from_numpy(l))\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(premise, char_premise, premise_mask, char_premise_mask, hypothesis, char_hypothesis, hypothesis_mask, char_hypothesis_mask,l)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 32, 1200])\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
