{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from data_iterator import TextIterator\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import pickle as pkl\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "num_words = 100140\n",
    "batch_size=32\n",
    "valid_batch_size=32\n",
    "dim_word = 300\n",
    "char_nout = 100\n",
    "dim_char_emb = 15\n",
    "learning_rate = 0.00001\n",
    "dim_hidden = 600\n",
    "max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp = Variable(torch.LongTensor([[2,1],[3,4]]).cuda())\n",
    "x= torch.zeros(3, 3)\n",
    "x[:2,1]=1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def str2list(s):\n",
    "    alphabet = \" abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    l = len(s)\n",
    "    ans = []\n",
    "    for i in range(0, l):\n",
    "        a = alphabet.find(s[i])\n",
    "        if a >= 0:\n",
    "            ans.append(a)\n",
    "        else:\n",
    "            ans.append(0)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(seqs_x, seqs_y, labels, worddicts_r, maxlen=None):\n",
    "    # x: a list of sentences\n",
    "    lengths_x = [len(s) for s in seqs_x]\n",
    "    lengths_y = [len(s) for s in seqs_y]\n",
    "\n",
    "    if maxlen is not None:\n",
    "        new_seqs_x = []\n",
    "        new_seqs_y = []\n",
    "        new_lengths_x = []\n",
    "        new_lengths_y = []\n",
    "        new_labels = []\n",
    "        for l_x, s_x, l_y, s_y, l in zip(lengths_x, seqs_x, lengths_y, seqs_y, labels):\n",
    "            if l_x < maxlen and l_y < maxlen:\n",
    "                new_seqs_x.append(s_x)\n",
    "                new_lengths_x.append(l_x)\n",
    "                new_seqs_y.append(s_y)\n",
    "                new_lengths_y.append(l_y)\n",
    "                new_labels.append(l)\n",
    "        lengths_x = new_lengths_x\n",
    "        seqs_x = new_seqs_x\n",
    "        lengths_y = new_lengths_y\n",
    "        seqs_y = new_seqs_y\n",
    "        labels = new_labels\n",
    "\n",
    "        if len(lengths_x) < 1 or len(lengths_y) < 1:\n",
    "            return None\n",
    "\n",
    "    max_char_len_x = 0\n",
    "    max_char_len_y = 0\n",
    "    seqs_x_char = []\n",
    "    l_seqs_x_char = []\n",
    "    seqs_y_char = []\n",
    "    l_seqs_y_char = []\n",
    "\n",
    "    for idx, [s_x, s_y, s_l] in enumerate(zip(seqs_x, seqs_y, labels)):\n",
    "        temp_seqs_x_char = []\n",
    "        temp_l_seqs_x_char = []\n",
    "        temp_seqs_y_char = []\n",
    "        temp_l_seqs_y_char = []\n",
    "        for w_x in s_x:\n",
    "            word = worddicts_r[w_x]\n",
    "            word_list = str2list(word)\n",
    "            l_word_list = len(word_list)\n",
    "            temp_seqs_x_char.append(word_list)\n",
    "            temp_l_seqs_x_char.append(l_word_list)\n",
    "            if l_word_list >= max_char_len_x:\n",
    "                max_char_len_x = l_word_list\n",
    "        for w_y in s_y:\n",
    "            word = worddicts_r[w_y]\n",
    "            word_list = str2list(word)\n",
    "            l_word_list = len(word_list)\n",
    "            temp_seqs_y_char.append(word_list)\n",
    "            temp_l_seqs_y_char.append(l_word_list)\n",
    "            if l_word_list >= max_char_len_y:\n",
    "                max_char_len_y = l_word_list\n",
    "\n",
    "        seqs_x_char.append(temp_seqs_x_char)\n",
    "        l_seqs_x_char.append(temp_l_seqs_x_char)\n",
    "        seqs_y_char.append(temp_seqs_y_char)\n",
    "        l_seqs_y_char.append(temp_l_seqs_y_char)\n",
    "\n",
    "    n_samples = len(seqs_x)\n",
    "    maxlen_x = max(lengths_x)\n",
    "    maxlen_y = max(lengths_y)\n",
    "    if torch.cuda.is_available():\n",
    "        x = torch.zeros(maxlen_x, n_samples).long().cuda()\n",
    "        y = torch.zeros(maxlen_y, n_samples).long().cuda()\n",
    "        x_mask = torch.zeros(maxlen_x, n_samples).cuda()\n",
    "        y_mask = torch.zeros(maxlen_y, n_samples).cuda()\n",
    "        l = torch.zeros(n_samples,).long().cuda()\n",
    "        char_x = torch.zeros(maxlen_x, n_samples, max_char_len_x).long().cuda()\n",
    "        char_x_mask = torch.zeros(maxlen_x, n_samples, max_char_len_x).cuda()\n",
    "        char_y = torch.zeros(maxlen_y, n_samples, max_char_len_y).long().cuda()\n",
    "        char_y_mask = torch.zeros(maxlen_y, n_samples, max_char_len_y).cuda()\n",
    "    else:\n",
    "        x = torch.zeros(maxlen_x, n_samples).long()\n",
    "        y = torch.zeros(maxlen_y, n_samples).long()\n",
    "        x_mask = torch.zeros(maxlen_x, n_samples)\n",
    "        y_mask = torch.zeros(maxlen_y, n_samples)\n",
    "        l = torch.zeros(n_samples,).long()\n",
    "        char_x = torch.zeros(maxlen_x, n_samples, max_char_len_x).long()\n",
    "        char_x_mask = torch.zeros(maxlen_x, n_samples, max_char_len_x)\n",
    "        char_y = torch.zeros(maxlen_y, n_samples, max_char_len_y).long()\n",
    "        char_y_mask = torch.zeros(maxlen_y, n_samples, max_char_len_y)\n",
    "\n",
    "    for idx, [s_x, s_y, ll] in enumerate(zip(seqs_x, seqs_y, labels)):\n",
    "        if torch.cuda.is_available():\n",
    "            x[:lengths_x[idx], idx] = torch.Tensor(s_x).cuda()\n",
    "            x_mask[:lengths_x[idx], idx]= 1.\n",
    "            y[:lengths_y[idx], idx] = torch.Tensor(s_y).cuda()\n",
    "            y_mask[:lengths_y[idx], idx] = 1.\n",
    "            l[idx] = int(ll)\n",
    "\n",
    "            for j in range(0, lengths_x[idx]):\n",
    "                char_x[j, idx, :l_seqs_x_char[idx][j]] = torch.Tensor(seqs_x_char[idx][j]).cuda()\n",
    "                char_x_mask[j, idx, :l_seqs_x_char[idx][j]] = 1.\n",
    "            for j in range(0, lengths_y[idx]):\n",
    "                char_y[j, idx, :l_seqs_y_char[idx][j]] = torch.Tensor(seqs_y_char[idx][j]).cuda()\n",
    "                char_y_mask[j, idx, :l_seqs_y_char[idx][j]] = 1.\n",
    "        else:\n",
    "            x[:lengths_x[idx], idx] = torch.Tensor(s_x)\n",
    "            x_mask[:lengths_x[idx], idx]= 1.\n",
    "            y[:lengths_y[idx], idx] = torch.Tensor(s_y)\n",
    "            y_mask[:lengths_y[idx], idx] = 1.\n",
    "            l[idx] = int(ll)\n",
    "\n",
    "            for j in range(0, lengths_x[idx]):\n",
    "                char_x[j, idx, :l_seqs_x_char[idx][j]] = torch.Tensor(seqs_x_char[idx][j])\n",
    "                char_x_mask[j, idx, :l_seqs_x_char[idx][j]] = 1.\n",
    "            for j in range(0, lengths_y[idx]):\n",
    "                char_y[j, idx, :l_seqs_y_char[idx][j]] = torch.Tensor(seqs_y_char[idx][j])\n",
    "                char_y_mask[j, idx, :l_seqs_y_char[idx][j]] = 1.\n",
    "\n",
    "    return Variable(x), Variable(x_mask), Variable(char_x), Variable(char_x_mask), Variable(y), Variable(y_mask), Variable(char_y), Variable(char_y_mask), Variable(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, nin, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        if torch.cuda.is_available():\n",
    "            self.linear_f = nn.Linear(nin + hidden_size, hidden_size).cuda()\n",
    "            self.linear_i = nn.Linear(nin + hidden_size, hidden_size).cuda()\n",
    "            self.linear_ctilde = nn.Linear(nin + hidden_size, hidden_size).cuda()\n",
    "            self.linear_o = nn.Linear(nin + hidden_size, hidden_size).cuda()\n",
    "            \n",
    "           \n",
    "        else:     \n",
    "            self.linear_f = nn.Linear(nin + hidden_size , hidden_size)\n",
    "            self.linear_i = nn.Linear(nin + hidden_size, hidden_size)\n",
    "            self.linear_ctilde = nn.Linear(nin + hidden_size, hidden_size)\n",
    "            self.linear_o = nn.Linear(nin + hidden_size , hidden_size)\n",
    "            \n",
    "\n",
    "        self.hidden_size = hidden_size \n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        #embs = torch.chunk(x, x.size()[1], 1)\n",
    "        hidden, c = self.init_hidden(x.size(1))\n",
    "        \n",
    "        def step(emb, hid, c_t_old, mask_cur):\n",
    "            combined = torch.cat((hid, emb), 1)\n",
    "           \n",
    "            f = F.sigmoid(self.linear_f(combined))\n",
    "            i = F.sigmoid(self.linear_i(combined))\n",
    "            o = F.sigmoid(self.linear_o(combined))\n",
    "            c_tilde = F.tanh(self.linear_ctilde(combined))\n",
    "            \n",
    "            c_t = f * c_t_old + i * c_tilde\n",
    "            c_t = mask_cur[:, None] * c_t + (1. - mask_cur)[:, None] * c_t_old\n",
    "            \n",
    "            hid_new = o * F.tanh(c_t)\n",
    "            hid_new = mask_cur[:, None] * hid_new + (1. - mask_cur)[:, None] * hid\n",
    "            \n",
    "            return hid_new, c_t, i\n",
    "        \n",
    "        h_hist = []\n",
    "        i_hist = []\n",
    "        for i in range(x.size(0)):\n",
    "            hidden, c, i = step(x[i].squeeze(), hidden, c, mask[i])     \n",
    "            h_hist.append(hidden[None, :, :])\n",
    "            i_hist.append(i[None, :, :])\n",
    "            \n",
    "        return torch.cat(h_hist), torch.cat(i_hist)\n",
    "\n",
    "    def init_hidden(self, bat_size):\n",
    "        if torch.cuda.is_available():\n",
    "            h0 = Variable(torch.zeros(bat_size, self.hidden_size).cuda())\n",
    "            c0 = Variable(torch.zeros(bat_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(bat_size, self.hidden_size))\n",
    "            c0 = Variable(torch.zeros(bat_size, self.hidden_size))\n",
    "        return h0, c0\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.linear_f, self.linear_i, self.linear_ctilde, self.linear_o]\n",
    "     \n",
    "        for layer in lin_layers:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            layer.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class NLI(nn.Module):\n",
    "    def __init__(self, dim_word, char_nout, dim_char_emb, word_embeddings_file, worddict, num_words, dim_hidden):\n",
    "        super(NLI, self).__init__()\n",
    "        self.dim_word = dim_word\n",
    "        self.char_nout = char_nout\n",
    "        self.dim_char_emb = dim_char_emb \n",
    "        self.char_k_cols = dim_char_emb\n",
    "        self.char_k_rows=[1,3,5]\n",
    "        self.hidden_size = dim_hidden\n",
    "        self.word_embeddings = self.create_word_embeddings(word_embeddings_file, worddict, num_words, dim_word)\n",
    "        \n",
    "        dim_emb = dim_word + 3*char_nout\n",
    "        self.alphabet = \" abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "\n",
    "        self.LSTM1 = LSTM(dim_emb, dim_hidden)\n",
    "        self.LSTM1_rev = LSTM(dim_emb, dim_hidden)\n",
    "        self.LSTM2 = LSTM(dim_emb+2*dim_hidden, dim_hidden)\n",
    "        self.LSTM2_rev = LSTM(dim_emb+2*dim_hidden, dim_hidden)\n",
    "        self.LSTM3 = LSTM(dim_emb+2*dim_hidden, dim_hidden)\n",
    "        self.LSTM3_rev = LSTM(dim_emb+2*dim_hidden, dim_hidden)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print('CUDA available')\n",
    "            self.drop = nn.Dropout(p=0.5).cuda()\n",
    "            self.conv1 = nn.Conv2d(1, self.char_nout, (self.char_k_rows[0], self.char_k_cols)).cuda()\n",
    "            self.conv2 = nn.Conv2d(1, self.char_nout, (self.char_k_rows[1], self.char_k_cols)).cuda()\n",
    "            self.conv3 = nn.Conv2d(1, self.char_nout, (self.char_k_rows[2], self.char_k_cols)).cuda()\n",
    "            self.Charemb = nn.Embedding(len(self.alphabet) + 1, self.dim_char_emb, padding_idx=0).cuda()\n",
    "            self.Linear1 = nn.Linear(24*dim_hidden, dim_hidden).cuda()\n",
    "            self.Linear2 = nn.Linear(25*dim_hidden, dim_hidden).cuda()\n",
    "            self.Linear3 = nn.Linear(dim_hidden, 3).cuda()\n",
    "        else:\n",
    "            self.Charemb = nn.Embedding(len(self.alphabet) + 1, self.dim_char_emb, padding_idx=0)\n",
    "            self.Linear1 = nn.Linear(24*dim_hidden, dim_hidden)\n",
    "            self.Linear2 = nn.Linear(25*dim_hidden, dim_hidden)\n",
    "            self.Linear3 = nn.Linear(dim_hidden, 3)\n",
    "            self.drop = nn.Dropout(p=0.5)\n",
    "            self.conv1 = nn.Conv2d(1, self.char_nout, (self.char_k_rows[0], self.char_k_cols))\n",
    "            self.conv2 = nn.Conv2d(1, self.char_nout, (self.char_k_rows[1], self.char_k_cols))\n",
    "            self.conv3 = nn.Conv2d(1, self.char_nout, (self.char_k_rows[2], self.char_k_cols))\n",
    "\n",
    "        \n",
    "        self.init_weights(dim_hidden)\n",
    "\n",
    "    def create_word_embeddings(self, file_name, worddicts, num_words, dim_word):\n",
    "        if torch.cuda.is_available():\n",
    "            word_embeddings = Variable(torch.zeros(num_words, dim_word).cuda())\n",
    "        else:\n",
    "            word_embeddings = Variable(torch.zeros(num_words, dim_word))\n",
    "        word_embeddings.data.normal_(0, 0.01)\n",
    "        with open(file_name, 'r') as f:\n",
    "            for line in f:\n",
    "                tmp = line.split()\n",
    "                word = tmp[0]\n",
    "                vector = tmp[1:]\n",
    "                len_vec = len(vector)\n",
    "                \n",
    "                if(len_vec>300):\n",
    "                    diff = len_vec-300\n",
    "                    word = word.join(vector[:diff])\n",
    "                    vector = vector[diff:]\n",
    "                    \n",
    "                    \n",
    "                if word in worddicts and worddicts[word] < num_words:\n",
    "                    vector = [float(x) for x in vector]\n",
    "                    if torch.cuda.is_available():\n",
    "                        word_embeddings[worddicts[word], :] = torch.FloatTensor(vector[0:300]).cuda()\n",
    "                    else:\n",
    "                        word_embeddings[worddicts[word], :] = torch.FloatTensor(vector[0:300])\n",
    "            \n",
    "        return word_embeddings\n",
    "        \n",
    "    def forward(self, premise, char_premise, premise_mask, char_premise_mask, hypothesis, char_hypothesis, hypothesis_mask, char_hypothesis_mask,l):\n",
    "        #premise = number of words * number of samples. Also hypothesis = number of words * number of samples\n",
    "        n_timesteps_premise = premise.size(0)\n",
    "        n_timesteps_hypothesis = hypothesis.size(0)\n",
    "        n_samples = premise.size(1)\n",
    "        \n",
    "        \n",
    "        premise_char_vector = self.compute_character_embeddings(char_premise, n_timesteps_premise, n_samples, char_premise_mask)\n",
    "        hypothesis_char_vector = self.compute_character_embeddings(char_hypothesis, n_timesteps_hypothesis, n_samples, char_hypothesis_mask)\n",
    "        \n",
    "\n",
    "        premise_word_emb = self.word_embeddings[premise.view(-1)].view(n_timesteps_premise, n_samples, self.dim_word)\n",
    "        hypothesis_word_emb = self.word_embeddings[hypothesis.view(-1)].view(n_timesteps_hypothesis, n_samples, self.dim_word)\n",
    "        \n",
    "        hypothesis_emb = torch.cat((hypothesis_word_emb, hypothesis_char_vector), 2)\n",
    "        hypothesis_emb = self.drop(hypothesis_emb)\n",
    "        \n",
    "        premise_emb = torch.cat((premise_word_emb, premise_char_vector), 2)\n",
    "        premise_emb = self.drop(premise_emb)\n",
    "    \n",
    "        premise_seq, premise_rev_seq = self.sequence_encoder(premise_emb, premise_mask)\n",
    "        hypothesis_seq, hypothesis_rev_seq = self.sequence_encoder(hypothesis_emb, hypothesis_mask)\n",
    "        \n",
    "        \n",
    "        premise_comp = self.make_composite_vector(premise_seq, premise_rev_seq, premise_mask)\n",
    "        hypothesis_comp = self.make_composite_vector(hypothesis_seq, hypothesis_rev_seq, hypothesis_mask)\n",
    "        \n",
    "    \n",
    "        logit_0 = torch.cat((premise_comp, hypothesis_comp, torch.abs(premise_comp - hypothesis_comp), premise_comp * hypothesis_comp),1)\n",
    "        logit = F.relu(self.Linear1(logit_0))\n",
    "        logit = self.drop(logit)\n",
    "        \n",
    "        logit = torch.cat((logit_0, logit), 1)\n",
    "        \n",
    "        logit = F.relu(self.Linear2(logit))\n",
    "        logit = self.drop(logit)\n",
    "        \n",
    "        logit = self.Linear3(logit)\n",
    "        \n",
    "        probs = F.softmax(logit)\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def sequence_encoder(self, emb, mask):\n",
    "        reverse_emb = self.reverseTensor(emb)\n",
    "        reverse_mask = self.reverseTensor(mask)\n",
    "        startl1 = time.time()\n",
    "        #  LSTM1\n",
    "        seq1 = self.LSTM1(emb, mask)\n",
    "        seq_reverse1 = self.LSTM1_rev(reverse_emb, reverse_mask)\n",
    "        \n",
    "        inp_seq2 = torch.cat((seq1[0], self.reverseTensor(seq_reverse1[0])), len(seq1[0].size()) - 1)\n",
    "        inp_seq2 = torch.cat((inp_seq2,emb),2)\n",
    "        reverse_inp_seq2 = self.reverseTensor(inp_seq2)\n",
    "\n",
    "        #  LSTM2\n",
    "        seq2 = self.LSTM2(inp_seq2, mask)\n",
    "        seq_reverse2 = self.LSTM2_rev(reverse_inp_seq2, reverse_mask)\n",
    "        \n",
    "        inp_seq3 = torch.cat((seq2[0], self.reverseTensor(seq_reverse2[0])), len(seq2[0].size()) - 1)\n",
    "        inp_seq3 = torch.cat((inp_seq3,emb),2)\n",
    "        reverse_inp_seq3 = self.reverseTensor(inp_seq3)\n",
    "\n",
    "\n",
    "        #  LSTM3\n",
    "        seq3 = self.LSTM3(inp_seq3, mask)\n",
    "        seq_reverse3 = self.LSTM3_rev(reverse_inp_seq3, reverse_mask)\n",
    "        \n",
    "        return seq3,seq_reverse3\n",
    "        \n",
    "    def make_composite_vector(self, seq, seq_rev, mask):\n",
    "        output = torch.cat((seq[0], self.reverseTensor(seq_rev[0])), len(seq[0].size()) - 1)\n",
    "        \n",
    "        gate = torch.cat((seq[1], self.reverseTensor(seq_rev[1])), len(seq[1].size()) - 1)\n",
    "        gate = gate.norm(2, 2)\n",
    "\n",
    "        mean = (output * mask[:, :, None]).sum(0) / (mask.sum(0)[:, None])\n",
    "        maxi = (output * mask[:, :, None]).max(0)[0]\n",
    "        gate_2 = (output * gate[:, :, None] * mask[:, :, None]).sum(0) / ((gate[:, :, None] * mask[:, :, None]).sum(0))\n",
    "        rep = torch.cat((mean, maxi, gate_2),1)\n",
    "        return rep\n",
    "\n",
    "    def reverseTensor(self, tensor):\n",
    "        idx = [i for i in range(tensor.size(0)-1, -1, -1)]\n",
    "        if torch.cuda.is_available():\n",
    "            idx = Variable(torch.LongTensor(idx).cuda())\n",
    "        else:\n",
    "            idx = Variable(torch.LongTensor(idx))\n",
    "        inverted_tensor = tensor.index_select(0, idx)\n",
    "        return inverted_tensor \n",
    "        \n",
    "    def compute_character_embeddings(self, chars_word, n_timesteps, num_samples, char_mask):\n",
    "        emb_char = self.Charemb(chars_word.view(-1)).view(n_timesteps, num_samples, chars_word.size(2), self.dim_char_emb)\n",
    "        emb_char = emb_char * char_mask[:, :, :, None]\n",
    "        emb_char_inp = emb_char.view(n_timesteps * num_samples, 1, chars_word.size(2), self.dim_char_emb)\n",
    "\n",
    "        char_level_emb1 = self.apply_filter_and_get_char_embedding(self.conv1, emb_char_inp, num_samples, n_timesteps)\n",
    "        char_level_emb2 = self.apply_filter_and_get_char_embedding(self.conv2, emb_char_inp, num_samples, n_timesteps)\n",
    "        char_level_emb3 = self.apply_filter_and_get_char_embedding(self.conv3, emb_char_inp, num_samples, n_timesteps)\n",
    "        \n",
    "        emb_chars = [char_level_emb1, char_level_emb2, char_level_emb3]\n",
    "        emb_char = torch.cat(emb_chars,2)\n",
    "        return emb_char\n",
    "    \n",
    "    def apply_filter_and_get_char_embedding(self, conv, emb_char_inp, n_samples, n_timesteps):\n",
    "        emb_char = conv(emb_char_inp)\n",
    "        emb_char = F.relu(emb_char)\n",
    "        emb_char = emb_char.view(n_timesteps * n_samples, self.char_nout, emb_char.size(2))\n",
    "        emb_char = emb_char.max(2)[0]\n",
    "        emb_char = emb_char.view(n_timesteps, n_samples, self.char_nout)\n",
    "        return emb_char\n",
    "        \n",
    "        \n",
    "    def init_weights(self, dim):\n",
    "        initrange = 0.1\n",
    "        self.init_convs(self.conv1, self.char_k_rows[0])\n",
    "        self.init_convs(self.conv2, self.char_k_rows[1])\n",
    "        self.init_convs(self.conv3, self.char_k_rows[2])\n",
    "        \n",
    "        self.Charemb.weight.data.uniform_(-initrange, initrange)\n",
    "        self.Linear1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.Linear1.bias.data.fill_(0)\n",
    "        self.Linear2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.Linear2.bias.data.fill_(0)\n",
    "        self.Linear3.weight.data.uniform_(-initrange, initrange)\n",
    "        self.Linear3.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "       \n",
    "    def init_convs(self, conv, char_k_row):\n",
    "        w_bound = math.sqrt(3 * char_k_row * self.char_k_cols)\n",
    "        conv.weight.data.uniform_(-1.0/w_bound, 1.0/w_bound)\n",
    "        conv.bias.data.fill_(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = TextIterator('word_sequence/premise_multinli_1.0_train.txt',\n",
    "                     'word_sequence/hypothesis_multinli_1.0_train.txt',\n",
    "                     'word_sequence/label_multinli_1.0_train.txt',\n",
    "                     'word_sequence/vocab_cased.pkl',\n",
    "                      n_words=num_words,\n",
    "                      batch_size=batch_size)\n",
    "valid = TextIterator('word_sequence/premise_multinli_1.0_dev_matched.txt',\n",
    "                     'word_sequence/hypothesis_multinli_1.0_dev_matched.txt',\n",
    "                     'word_sequence/label_multinli_1.0_dev_matched.txt',\n",
    "                     'word_sequence/vocab_cased.pkl',\n",
    "                      n_words=num_words,\n",
    "                      batch_size=valid_batch_size,\n",
    "                      shuffle=False)\n",
    "\n",
    "test = TextIterator('word_sequence/premise_multinli_1.0_dev_mismatched.txt',\n",
    "                    'word_sequence/hypothesis_multinli_1.0_dev_mismatched.txt',\n",
    "                    'word_sequence/label_multinli_1.0_dev_mismatched.txt',\n",
    "                    'word_sequence/vocab_cased.pkl',\n",
    "                    n_words=num_words,\n",
    "                    batch_size=valid_batch_size,\n",
    "                    shuffle=False)\n",
    "\n",
    "with open('word_sequence/vocab_cased.pkl', 'rb') as f:\n",
    "    worddicts = pkl.load(f)\n",
    "worddicts_r = dict()\n",
    "\n",
    "for kk, vv in worddicts.items():\n",
    "    worddicts_r[vv] = kk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pred_acc( iterator, worddicts_r):\n",
    "    model.eval()\n",
    "    valid_acc = 0\n",
    "    n_done = 0\n",
    "    num_times = 10\n",
    "\n",
    "    for x1, x2, y in iterator:\n",
    "        n_done += len(x1)\n",
    "        tp1 = time.time()\n",
    "        premise, premise_mask, char_premise, char_premise_mask, hypothesis, hypothesis_mask, char_hypothesis, char_hypothesis_mask, l = prepare_data(x1,x2,y,worddicts_r, max_len)\n",
    "        tp2 = time.time()\n",
    "        print(tp2-tp1)\n",
    "        outputs = model(premise, char_premise, premise_mask, char_premise_mask, hypothesis, char_hypothesis, hypothesis_mask, char_hypothesis_mask,l)\n",
    "        valid_acc += (outputs.max(1)[1] == l).sum().data[0]\n",
    "    valid_acc = 1.0 * valid_acc / n_done\n",
    "    return valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = NLI(dim_word, char_nout, dim_char_emb, 'data/glove.840B.300d.txt', worddicts, num_words, dim_hidden).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)  \n",
    "print('training')\n",
    "import time\n",
    "\n",
    "loss = nn.CrossEntropyLoss() \n",
    "uidx =0\n",
    "num_epochs = 10\n",
    "validFreq = 5\n",
    "eval_period = 1000\n",
    "best_acc = 0.0 \n",
    "model_path = 'ADAM_00001'\n",
    "for epoch in range(num_epochs):\n",
    "    for x1, x2, y in train:        \n",
    "        model.train()\n",
    "        uidx += 1\n",
    "        #t1 = time.time()\n",
    "        premise, premise_mask, char_premise, char_premise_mask, hypothesis, hypothesis_mask, char_hypothesis, char_hypothesis_mask, l = prepare_data(x1,x2,y,worddicts_r, max_len)\n",
    "        #t2 = time.time()\n",
    "        \n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #t3 = time.time()\n",
    "        outputs = model(premise, char_premise, premise_mask, char_premise_mask, hypothesis, char_hypothesis, hypothesis_mask, char_hypothesis_mask,l)   \n",
    "        \n",
    "        #t4 = time.time()\n",
    "        \n",
    "        lossy = loss(outputs, l)\n",
    "        #t5 = time.time()\n",
    "        \n",
    "        lossy.backward()\n",
    "        #t6 = time.time()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 10.0)\n",
    "\n",
    "        #t7 = time.time()\n",
    "        optimizer.step()\n",
    "        #t8 = time.time()\n",
    "        #print('time to create data', t2-t1)\n",
    "        #print('time to zero grad', t3-t2)\n",
    "        #print('time to forward', t4-t3)\n",
    "        #print('time to compute loss', t5-t4)\n",
    "        #print('time to backward', t6-t5)\n",
    "        #print('time to clip', t7-t6)\n",
    "        #print('time to step', t8-t7)\n",
    "        \n",
    "        print(uidx,lossy.data[0])\n",
    "        if uidx%eval_period == 0:\n",
    "            print(outputs)\n",
    "            valid_acc = pred_acc(valid, worddicts_r)\n",
    "            print('Epoch ',epoch, ' Valid Accuracy = ', valid_acc)\n",
    "            if(valid_acc>best_acc):\n",
    "                torch.save(model, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'SGD_0.01_3000iter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "valid_acc = pred_acc(valid, worddicts_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
