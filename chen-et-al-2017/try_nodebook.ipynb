{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from data_iterator import TextIterator\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import pickle as pkl\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_words = 100140\n",
    "batch_size=32\n",
    "valid_batch_size=32\n",
    "dim_word = 300\n",
    "char_nout = 100\n",
    "dim_char_emb = 15\n",
    "learning_rate = 0.01\n",
    "dim_hidden = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = Variable(torch.Tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]]))\n",
    "\n",
    "y = [1]\n",
    "Variable(torch.Tensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str2list(s):\n",
    "    alphabet = \" abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    l = len(s)\n",
    "    ans = []\n",
    "    for i in range(0, l):\n",
    "        a = alphabet.find(s[i])\n",
    "        if a >= 0:\n",
    "            ans.append(a)\n",
    "        else:\n",
    "            ans.append(0)\n",
    "            #print(s[i])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(seqs_x, seqs_y, labels, worddicts_r, maxlen=None):\n",
    "    # x: a list of sentences\n",
    "    lengths_x = [len(s) for s in seqs_x]\n",
    "    lengths_y = [len(s) for s in seqs_y]\n",
    "\n",
    "    if maxlen is not None:\n",
    "        new_seqs_x = []\n",
    "        new_seqs_y = []\n",
    "        new_lengths_x = []\n",
    "        new_lengths_y = []\n",
    "        new_labels = []\n",
    "        for l_x, s_x, l_y, s_y, l in zip(lengths_x, seqs_x, lengths_y, seqs_y, labels):\n",
    "            if l_x < maxlen and l_y < maxlen:\n",
    "                new_seqs_x.append(s_x)\n",
    "                new_lengths_x.append(l_x)\n",
    "                new_seqs_y.append(s_y)\n",
    "                new_lengths_y.append(l_y)\n",
    "                new_labels.append(l)\n",
    "        lengths_x = new_lengths_x\n",
    "        seqs_x = new_seqs_x\n",
    "        lengths_y = new_lengths_y\n",
    "        seqs_y = new_seqs_y\n",
    "        labels = new_labels\n",
    "\n",
    "        if len(lengths_x) < 1 or len(lengths_y) < 1:\n",
    "            return None\n",
    "\n",
    "    max_char_len_x = 0\n",
    "    max_char_len_y = 0\n",
    "    seqs_x_char = []\n",
    "    l_seqs_x_char = []\n",
    "    seqs_y_char = []\n",
    "    l_seqs_y_char = []\n",
    "\n",
    "    for idx, [s_x, s_y, s_l] in enumerate(zip(seqs_x, seqs_y, labels)):\n",
    "        temp_seqs_x_char = []\n",
    "        temp_l_seqs_x_char = []\n",
    "        temp_seqs_y_char = []\n",
    "        temp_l_seqs_y_char = []\n",
    "        for w_x in s_x:\n",
    "            word = worddicts_r[w_x]\n",
    "            word_list = str2list(word)\n",
    "            l_word_list = len(word_list)\n",
    "            temp_seqs_x_char.append(word_list)\n",
    "            temp_l_seqs_x_char.append(l_word_list)\n",
    "            if l_word_list >= max_char_len_x:\n",
    "                max_char_len_x = l_word_list\n",
    "        for w_y in s_y:\n",
    "            word = worddicts_r[w_y]\n",
    "            word_list = str2list(word)\n",
    "            l_word_list = len(word_list)\n",
    "            temp_seqs_y_char.append(word_list)\n",
    "            temp_l_seqs_y_char.append(l_word_list)\n",
    "            if l_word_list >= max_char_len_y:\n",
    "                max_char_len_y = l_word_list\n",
    "\n",
    "        seqs_x_char.append(temp_seqs_x_char)\n",
    "        l_seqs_x_char.append(temp_l_seqs_x_char)\n",
    "        seqs_y_char.append(temp_seqs_y_char)\n",
    "        l_seqs_y_char.append(temp_l_seqs_y_char)\n",
    "\n",
    "    n_samples = len(seqs_x)\n",
    "    maxlen_x = numpy.max(lengths_x)\n",
    "    maxlen_y = numpy.max(lengths_y)\n",
    "\n",
    "    x = numpy.zeros((maxlen_x, n_samples)).astype('int64')\n",
    "    y = numpy.zeros((maxlen_y, n_samples)).astype('int64')\n",
    "    x_mask = numpy.zeros((maxlen_x, n_samples)).astype('float32')\n",
    "    y_mask = numpy.zeros((maxlen_y, n_samples)).astype('float32')\n",
    "    l = numpy.zeros((n_samples,)).astype('int64')\n",
    "    char_x = numpy.zeros((maxlen_x, n_samples, max_char_len_x)).astype('int64')\n",
    "    char_x_mask = numpy.zeros((maxlen_x, n_samples, max_char_len_x)).astype('float32')\n",
    "    char_y = numpy.zeros((maxlen_y, n_samples, max_char_len_y)).astype('int64')\n",
    "    char_y_mask = numpy.zeros((maxlen_y, n_samples, max_char_len_y)).astype('float32')\n",
    "\n",
    "    for idx, [s_x, s_y, ll] in enumerate(zip(seqs_x, seqs_y, labels)):\n",
    "        x[:lengths_x[idx], idx] = s_x\n",
    "        x_mask[:lengths_x[idx], idx] = 1.\n",
    "        y[:lengths_y[idx], idx] = s_y\n",
    "        y_mask[:lengths_y[idx], idx] = 1.\n",
    "        l[idx] = ll\n",
    "\n",
    "        for j in range(0, lengths_x[idx]):\n",
    "            char_x[j, idx, :l_seqs_x_char[idx][j]] = seqs_x_char[idx][j]\n",
    "            char_x_mask[j, idx, :l_seqs_x_char[idx][j]] = 1.\n",
    "        for j in range(0, lengths_y[idx]):\n",
    "            char_y[j, idx, :l_seqs_y_char[idx][j]] = seqs_y_char[idx][j]\n",
    "            char_y_mask[j, idx, :l_seqs_y_char[idx][j]] = 1.\n",
    "\n",
    "    return x, x_mask, char_x, char_x_mask, y, y_mask, char_y, char_y_mask, l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, nin, dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # input weights\n",
    "        self.W = Parameter(torch.Tensor(nin, 4*dim))\n",
    "        \n",
    "        # for the previous hidden activation\n",
    "        self.U = Parameter(torch.Tensor(dim, 4*dim))\n",
    "\n",
    "        self.b = Parameter(torch.Tensor(4*dim,))\n",
    "        \n",
    "        \n",
    "        self.init_params(nin, dim)\n",
    "\n",
    "    def init_params(self, nin, dim):\n",
    "        self.W.data = torch.from_numpy(numpy.concatenate([self.norm_weight(nin, dim),\n",
    "                               self.norm_weight(nin, dim),\n",
    "                               self.norm_weight(nin, dim),\n",
    "                               self.norm_weight(nin, dim)], axis=1))\n",
    "\n",
    "        self.U.data = torch.from_numpy(numpy.concatenate([self.ortho_weight(dim),\n",
    "                               self.ortho_weight(dim),\n",
    "                               self.ortho_weight(dim),\n",
    "                               self.ortho_weight(dim)], axis=1))\n",
    "        \n",
    "        self.b.data = torch.from_numpy(numpy.zeros((4 * dim,)).astype('float32'))\n",
    "                                                        \n",
    "    def slice_d(self, _x, n, dim):\n",
    "        if len(_x.size()) == 3:\n",
    "            return _x[:, :, n * dim:(n + 1) * dim]\n",
    "        elif len(_x.size()) == 2:\n",
    "            return _x[:, n * dim:(n + 1) * dim]\n",
    "        return _x[n * dim:(n + 1) * dim]\n",
    "\n",
    "\n",
    "    # one time step of the lstm\n",
    "    def step(self, m_, x_, h_, c_, dim):\n",
    "        preact = torch.mm(h_, self.U)\n",
    "        preact += x_\n",
    "\n",
    "\n",
    "        i = F.sigmoid(self.slice_d(preact, 0, dim))\n",
    "        f = F.sigmoid(self.slice_d(preact, 1, dim))\n",
    "        o = F.sigmoid(self.slice_d(preact, 2, dim))\n",
    "        c = F.tanh(self.slice_d(preact, 3, dim))\n",
    "\n",
    "        c = f * c_ + i * c\n",
    "        c = m_[:, None] * c + (1. - m_)[:, None] * c_\n",
    "\n",
    "        h = o * F.tanh(c)\n",
    "        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n",
    "\n",
    "        return h, c, i, f, o, preact\n",
    "    \n",
    "\n",
    "    \n",
    "    # This function implements the lstm fprop\n",
    "    def forward(self, state_below, mask):\n",
    "        nsteps = state_below.size(0)\n",
    "        dim = self.U.size(0)\n",
    "\n",
    "        n_samples = state_below.size(1)\n",
    "        init_state = Variable(torch.zeros(n_samples, dim))\n",
    "        init_memory = Variable(torch.zeros(n_samples, dim))\n",
    "        state_below = torch.bmm(state_below, self.W.unsqueeze(0).expand(state_below.size(0), *self.W.size())) + self.b\n",
    "        \n",
    "        h_hist = []\n",
    "        i_hist = []\n",
    "        for current_step in range(nsteps):\n",
    "            if(current_step==0):\n",
    "                h_last, c_last, i_last,_,_,_ = self.step(mask[current_step], state_below[current_step], init_state, init_memory, dim)\n",
    "            else :\n",
    "                h_last, c_last, i_last,_,_,_ = self.step(mask[current_step], state_below[current_step], h_last, c_last, dim)\n",
    "\n",
    "           \n",
    "            h_hist.append(h_last[None, :, :])\n",
    "            i_hist.append(i_last[None, :, :])\n",
    "\n",
    "        return torch.cat(h_hist), torch.cat(i_hist)\n",
    "\n",
    "    # use the slice to calculate all the different gates\n",
    "    def norm_weight(self, nin, nout=None, scale=0.01, ortho=True):\n",
    "        if nout is None:\n",
    "            nout = nin\n",
    "        if nout == nin and ortho:\n",
    "            W = self.ortho_weight(nin)\n",
    "        else:\n",
    "            W = scale * numpy.random.randn(nin, nout)\n",
    "        return W.astype('float32')\n",
    "\n",
    "    def ortho_weight(self, ndim):\n",
    "        W = numpy.random.randn(ndim, ndim)\n",
    "        u, s, v = numpy.linalg.svd(W)\n",
    "        return u.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class NLI(nn.Module):\n",
    "    def __init__(self, dim_word, char_nout, dim_char_emb, word_embeddings_file, worddict, num_words, dim_hidden):\n",
    "        super(NLI, self).__init__()\n",
    "        self.dim_word = dim_word\n",
    "        self.char_nout = char_nout\n",
    "        self.dim_char_emb = dim_char_emb \n",
    "        self.char_k_cols = dim_char_emb\n",
    "        self.char_k_rows=[1,3,5]\n",
    "        self.hidden_size = dim_hidden\n",
    "        self.word_embeddings = self.create_word_embeddings(word_embeddings_file, worddict, num_words, dim_word)\n",
    "        \n",
    "        dim_emb = dim_word + 3*char_nout\n",
    "        self.alphabet = \" abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "        self.filter1 = None\n",
    "        self.filter2 = None\n",
    "        self.filter3 = None\n",
    "        \n",
    "        self.LSTM1 = LSTM(dim_emb, dim_hidden)\n",
    "        self.LSTM2 = LSTM(dim_emb+2*dim_hidden, dim_hidden)\n",
    "        self.LSTM3 = LSTM(dim_emb+2*dim_hidden, dim_hidden)\n",
    "        \n",
    "        self.Linear1 = nn.Linear(24*dim_hidden, dim_hidden)\n",
    "        self.Linear2 = nn.Linear(25*dim_hidden, dim_hidden)\n",
    "        self.Linear3 = nn.Linear(dim_hidden, 3)\n",
    "\n",
    "        self.init_weights(dim_hidden)\n",
    "\n",
    "    def create_word_embeddings(self, file_name, worddicts, num_words, dim_word):\n",
    "        word_embeddings = Variable(torch.from_numpy(self.norm_weight(num_words, dim_word)))\n",
    "        \n",
    "        with open(file_name, 'r') as f:\n",
    "            for line in f:\n",
    "                tmp = line.split()\n",
    "                word = tmp[0]\n",
    "                vector = tmp[1:]\n",
    "                len_vec = len(vector)\n",
    "                \n",
    "                if(len_vec>300):\n",
    "                    diff = len_vec-300\n",
    "                    word = word.join(vector[:diff])\n",
    "                    vector = vector[diff:]\n",
    "                    \n",
    "                    \n",
    "                if word in worddicts and worddicts[word] < num_words:\n",
    "                    vector = [float(x) for x in vector]\n",
    "                    word_embeddings[worddicts[word], :] = torch.FloatTensor(vector[0:300])\n",
    "            \n",
    "        return word_embeddings\n",
    "        \n",
    "    def forward(self, premise, char_premise, premise_mask, char_premise_mask, hypothesis, char_hypothesis, hypothesis_mask, char_hypothesis_mask,l,y):\n",
    "        #premise = number of words * number of samples. Also hypothesis = number of words * number of samples\n",
    "        n_timesteps_premise = premise.size(0)\n",
    "        n_timesteps_hypothesis = hypothesis.size(0)\n",
    "        n_samples = premise.size(1)\n",
    "        \n",
    "        premise_char_vector = self.compute_character_embeddings(char_premise, n_timesteps_premise, n_samples, char_premise_mask)\n",
    "        hypothesis_char_vector = self.compute_character_embeddings(char_hypothesis, n_timesteps_hypothesis, n_samples, char_hypothesis_mask)\n",
    "\n",
    "        premise_word_emb = self.word_embeddings[premise.view(-1)].view(n_timesteps_premise, n_samples, self.dim_word)\n",
    "        hypothesis_word_emb = self.word_embeddings[hypothesis.view(-1)].view(n_timesteps_hypothesis, n_samples, self.dim_word)\n",
    "        \n",
    "        hypothesis_emb = torch.cat([hypothesis_word_emb, hypothesis_char_vector], 2)\n",
    "        premise_emb = torch.cat([premise_word_emb, premise_char_vector], 2)\n",
    "        \n",
    "    \n",
    "        premise_seq, premise_rev_seq = self.sequence_encoder(premise_emb, premise_mask)\n",
    "        hypothesis_seq, hypothesis_rev_seq = self.sequence_encoder(hypothesis_emb, hypothesis_mask)\n",
    "        \n",
    "        premise_comp = self.make_composite_vector(premise_seq, premise_rev_seq, premise_mask)\n",
    "        hypothesis_comp = self.make_composite_vector(hypothesis_seq, hypothesis_rev_seq, hypothesis_mask)\n",
    "    \n",
    "        logit_0 = torch.cat([premise_comp, hypothesis_comp, torch.abs(premise_comp - hypothesis_comp), premise_comp * hypothesis_comp],1)\n",
    "        logit = F.relu(self.Linear1(logit_0))\n",
    "        logit = torch.cat([logit_0, logit], 1)\n",
    "        logit = F.relu(self.Linear2(logit))\n",
    "        logit = self.Linear3(logit)\n",
    "        probs = F.softmax(logit)\n",
    "        \n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        return probs\n",
    "    \n",
    "    def sequence_encoder(self, emb, mask):\n",
    "        reverse_emb = self.reverseTensor(emb)\n",
    "        reverse_mask = self.reverseTensor(mask)\n",
    "        \n",
    "        #  LSTM1\n",
    "        seq1 = self.LSTM1(emb, mask)\n",
    "        seq_reverse1 = self.LSTM1(reverse_emb, reverse_mask)\n",
    "        \n",
    "        inp_seq2 = torch.cat([seq1[0], self.reverseTensor(seq_reverse1[0])], len(seq1[0].size()) - 1)\n",
    "        inp_seq2 = torch.cat([inp_seq2,emb],2)\n",
    "        reverse_inp_seq2 = self.reverseTensor(inp_seq2)\n",
    "        \n",
    "        #  LSTM2\n",
    "        seq2 = self.LSTM2(inp_seq2, mask)\n",
    "        seq_reverse2 = self.LSTM2(reverse_inp_seq2, reverse_mask)\n",
    "        \n",
    "        inp_seq3 = torch.cat([seq2[0], self.reverseTensor(seq_reverse2[0])], len(seq2[0].size()) - 1)\n",
    "        inp_seq3 = torch.cat([inp_seq3,emb],2)\n",
    "        reverse_inp_seq3 = self.reverseTensor(inp_seq3)\n",
    "\n",
    "        #  LSTM3\n",
    "        seq3 = self.LSTM3(inp_seq3, mask)\n",
    "        seq_reverse3 = self.LSTM3(reverse_inp_seq3, reverse_mask)\n",
    "        \n",
    "        return seq3,seq_reverse3\n",
    "        \n",
    "    def make_composite_vector(self, seq, seq_rev, mask):\n",
    "        output = torch.cat([seq[0], self.reverseTensor(seq_rev[0])], len(seq[0].size()) - 1)\n",
    "        \n",
    "        gate = torch.cat([seq[1], self.reverseTensor(seq_rev[1])], len(seq[1].size()) - 1)\n",
    "        gate = gate.norm(2, 2)\n",
    "\n",
    "        mean = (output * mask[:, :, None]).sum(0) / mask.sum(0)[:, None]\n",
    "        maxi = (output * mask[:, :, None]).max(0)[0]\n",
    "        gate_2 = (output * gate[:, :, None] * mask[:, :, None]).sum(0) / (gate[:, :, None] * mask[:, :, None]).sum(0)\n",
    "        rep = torch.cat([mean, maxi, gate_2],1)\n",
    "        return rep\n",
    "\n",
    "    def reverseTensor(self, tensor):\n",
    "        idx = [i for i in range(tensor.size(0)-1, -1, -1)]\n",
    "        idx = Variable(torch.LongTensor(idx))\n",
    "        inverted_tensor = tensor.index_select(0, idx)\n",
    "        return inverted_tensor \n",
    "        \n",
    "    def compute_character_embeddings(self, chars_word, n_timesteps, num_samples, char_mask):\n",
    "        emb_char = self.Charemb[chars_word.view(-1)].view(n_timesteps, num_samples, chars_word.size(2), self.dim_char_emb)\n",
    "        emb_char = emb_char * char_mask[:, :, :, None]\n",
    "        emb_char_inp = emb_char.view(n_timesteps * num_samples, 1, chars_word.size(2), self.dim_char_emb)\n",
    "\n",
    "        char_level_emb1 = self.apply_filter_and_get_char_embedding(self.filter1, emb_char_inp, num_samples, n_timesteps)\n",
    "        char_level_emb2 = self.apply_filter_and_get_char_embedding(self.filter2, emb_char_inp, num_samples, n_timesteps)\n",
    "        char_level_emb3 = self.apply_filter_and_get_char_embedding(self.filter3, emb_char_inp, num_samples, n_timesteps)\n",
    "        \n",
    "        emb_chars = [char_level_emb1, char_level_emb2, char_level_emb3]\n",
    "        emb_char = torch.cat(emb_chars,2)\n",
    "        return emb_char\n",
    "    \n",
    "    def apply_filter_and_get_char_embedding(self, filter_type, emb_char_inp, n_samples, n_timesteps):\n",
    "        emb_char = F.conv2d(emb_char_inp, filter_type)\n",
    "        emb_char = F.relu(emb_char)\n",
    "        emb_char = emb_char.view(n_timesteps * n_samples, self.char_nout, emb_char.size(2))\n",
    "        emb_char = emb_char.max(2)[0]\n",
    "        emb_char = emb_char.view(n_timesteps, n_samples, self.char_nout)\n",
    "        return emb_char\n",
    "        \n",
    "        \n",
    "    def init_weights(self, dim):\n",
    "        initrange = 0.1\n",
    "        self.Charemb = Variable(torch.from_numpy(self.norm_weight(len(self.alphabet) + 1, self.dim_char_emb)), requires_grad = True)\n",
    "        self.filter1 = self.getFilter(self.char_k_rows[0])\n",
    "        self.filter2 = self.getFilter(self.char_k_rows[1])\n",
    "        self.filter3 = self.getFilter(self.char_k_rows[2])\n",
    "        \n",
    "        \n",
    "        self.Linear1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.Linear1.bias.data.fill_(0)\n",
    "        self.Linear2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.Linear2.bias.data.fill_(0)\n",
    "        self.Linear3.weight.data.uniform_(-initrange, initrange)\n",
    "        self.Linear3.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "       \n",
    "    def getFilter(self, char_k_row):\n",
    "        w_shp = (self.char_nout, 1, char_k_row, self.char_k_cols)\n",
    "        w_bound = numpy.sqrt(3 * char_k_row * self.char_k_cols)\n",
    "        return Variable(torch.from_numpy(numpy.random.uniform(low=-1.0/w_bound, high=1.0/w_bound, size=w_shp).astype('float32')), requires_grad = True)\n",
    "\n",
    "\n",
    "\n",
    "    def norm_weight(self, nin, nout=None, scale=0.01, ortho=True):\n",
    "        if nout is None:\n",
    "            nout = nin\n",
    "        if nout == nin and ortho:\n",
    "            W = self.ortho_weight(nin)\n",
    "        else:\n",
    "            W = scale * numpy.random.randn(nin, nout)\n",
    "        return W.astype('float32')\n",
    "\n",
    "    def ortho_weight(self, ndim):\n",
    "        W = numpy.random.randn(ndim, ndim)\n",
    "        u, s, v = numpy.linalg.svd(W)\n",
    "        return u.astype('float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = TextIterator('word_sequence/premise_multinli_1.0_train.txt',\n",
    "                     'word_sequence/hypothesis_multinli_1.0_train.txt',\n",
    "                     'word_sequence/label_multinli_1.0_train.txt',\n",
    "                     'word_sequence/vocab_cased.pkl',\n",
    "                      n_words=num_words,\n",
    "                      batch_size=batch_size)\n",
    "valid = TextIterator('word_sequence/premise_multinli_1.0_dev_matched.txt',\n",
    "                     'word_sequence/hypothesis_multinli_1.0_dev_matched.txt',\n",
    "                     'word_sequence/label_multinli_1.0_dev_matched.txt',\n",
    "                     'word_sequence/vocab_cased.pkl',\n",
    "                      n_words=num_words,\n",
    "                      batch_size=valid_batch_size,\n",
    "                      shuffle=False)\n",
    "\n",
    "test = TextIterator('word_sequence/premise_multinli_1.0_dev_mismatched.txt',\n",
    "                    'word_sequence/hypothesis_multinli_1.0_dev_mismatched.txt',\n",
    "                    'word_sequence/label_multinli_1.0_dev_mismatched.txt',\n",
    "                    'word_sequence/vocab_cased.pkl',\n",
    "                    n_words=num_words,\n",
    "                    batch_size=valid_batch_size,\n",
    "                    shuffle=False)\n",
    "\n",
    "with open('word_sequence/vocab_cased.pkl', 'rb') as f:\n",
    "    worddicts = pkl.load(f)\n",
    "worddicts_r = dict()\n",
    "\n",
    "for kk, vv in worddicts.items():\n",
    "    worddicts_r[vv] = kk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "Variable containing:\n",
      " 1.1003\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2077\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.0514\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2702\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.0514\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2702\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2389\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.1452\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2077\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.1452\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2077\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2077\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2702\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2389\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.2389\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1.3639\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6135cafc9b2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mlossy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlossy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mlossy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;31m#torch.nn.utils.clip_grad_norm(model.parameters(), 5.0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bhaskar.gurram/anaconda/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bhaskar.gurram/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model = NLI(dim_word, char_nout, dim_char_emb, 'data/glove.840B.300d.txt', worddicts, num_words, dim_hidden)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "print('training')\n",
    "loss = nn.CrossEntropyLoss() \n",
    "\n",
    "for x1, x2, y in train:\n",
    "\n",
    "    premise, premise_mask, char_premise, char_premise_mask, hypothesis, hypothesis_mask, char_hypothesis, char_hypothesis_mask, l = prepare_data(x1,x2,y,worddicts_r)\n",
    "    premise = Variable(torch.from_numpy(premise))\n",
    "    premise_mask = Variable(torch.from_numpy(premise_mask))\n",
    "    char_premise = Variable(torch.from_numpy(char_premise))\n",
    "    char_premise_mask = Variable(torch.from_numpy(char_premise_mask))\n",
    "    hypothesis = Variable(torch.from_numpy(hypothesis))\n",
    "    hypothesis_mask = Variable(torch.from_numpy(hypothesis_mask))\n",
    "    char_hypothesis = Variable(torch.from_numpy(char_hypothesis))\n",
    "    char_hypothesis_mask = Variable(torch.from_numpy(char_hypothesis_mask))\n",
    "    l = Variable(torch.from_numpy(l))\n",
    "    y = [int(data) for data in y]\n",
    "    y = Variable(torch.LongTensor(y).squeeze())\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(premise, char_premise, premise_mask, char_premise_mask, hypothesis, char_hypothesis, hypothesis_mask, char_hypothesis_mask,l,y)\n",
    "    lossy = loss(outputs, y)\n",
    "    print(lossy)\n",
    "    lossy.backward()\n",
    "    #torch.nn.utils.clip_grad_norm(model.parameters(), 5.0)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
