{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext.vocab as vocab\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import argparse\n",
    "import jsonlines\n",
    "import ujson as json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import shutil\n",
    "from collections import Counter, defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "UNKNOWN = '**UNK**'\n",
    "PADDING = '**PAD**'\n",
    "GO = '**GO**'  # it's called \"GO\" but actually serves as a null alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = argparse.Namespace()\n",
    "params.projection_dim = 200\n",
    "params.batch_size = 32\n",
    "params.max_len = 100\n",
    "params.hidden_dim = params.projection_dim\n",
    "params.train_file = \"/home/as10656/nli-batch-optimizations/multinli_1.0/multinli_1.0_train.jsonl\"\n",
    "params.train_small_file = \"/home/as10656/nli-batch-optimizations/multinli_1.0/multinli_1.0_train_small.jsonl\"\n",
    "params.dev_file = \"/home/as10656/nli-batch-optimizations/multinli_1.0/multinli_1.0_dev_matched.jsonl\"\n",
    "params.dev_mistmatched_file = \"/home/as10656/nli-batch-optimizations/multinli_1.0/multinli_1.0_dev_mismatched.jsonl\"\n",
    "params.lr = 0.0001\n",
    "params.seed = 7\n",
    "params.gru_encode = False\n",
    "params.dropout = 0.2\n",
    "params.nr_classes = 3\n",
    "params.epochs = 300\n",
    "params.patience = 20\n",
    "params.resume = False\n",
    "params.extra_debug = False\n",
    "params.use_optimizations = True\n",
    "params.use_intra_attention = True\n",
    "torch.manual_seed(params.seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modified version of utils and ioutils classes from https://github.com/erickrf/multiffn-nli\n",
    "class NLIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sentences1, sentences2, sizes1, sizes2, labels):\n",
    "        \"\"\"\n",
    "        :param sentences1: A 2D numpy array with sentences (the first in each\n",
    "            pair) composed of token indices\n",
    "        :param sentences2: Same as above for the second sentence in each pair\n",
    "        :param sizes1: A 1D numpy array with the size of each sentence in the\n",
    "            first group. Sentences should be filled with the PADDING token after\n",
    "            that point\n",
    "        :param sizes2: Same as above\n",
    "        :param labels: 1D numpy array with labels as integers\n",
    "        \"\"\"\n",
    "        self.sentences1 = sentences1\n",
    "        self.sentences2 = sentences2\n",
    "        self.sizes1 = sizes1\n",
    "        self.sizes2 = sizes2\n",
    "        self.labels = labels\n",
    "        self.num_items = len(sentences1)\n",
    "\n",
    "    def shuffle_data(self):\n",
    "        \"\"\"\n",
    "        Shuffle all data using the same random sequence.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        shuffle_arrays(self.sentences1, self.sentences2,\n",
    "                       self.sizes1, self.sizes2, self.labels)\n",
    "\n",
    "    def get_batch(self, from_, to):\n",
    "        \"\"\"\n",
    "        Return an NLIDataset object with the subset of the data contained in\n",
    "        the given interval. Note that the actual number of items may be less\n",
    "        than (`to` - `from_`) if there are not enough of them.\n",
    "        :param from_: which position to start from\n",
    "        :param to: which position to end\n",
    "        :return: an NLIDataset object\n",
    "        \"\"\"\n",
    "        if from_ == 0 and to >= self.num_items:\n",
    "            return self\n",
    "\n",
    "        subset = NLIDataset(self.sentences1[from_:to],\n",
    "                            self.sentences2[from_:to],\n",
    "                            self.sizes1[from_:to],\n",
    "                            self.sizes2[from_:to],\n",
    "                            self.labels[from_:to])\n",
    "        return subset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_items\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences1[idx], self.sentences2[idx], self.labels[idx]\n",
    "    \n",
    "\n",
    "class BatchedNLIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, pairs, word_dict, sizes1, sizes2, label_dict):\n",
    "        self.pairs = pairs\n",
    "        self.label_dict = label_dict\n",
    "        self.word_dict = word_dict\n",
    "        self.max_len1 = sizes1\n",
    "        self.max_len2 = sizes2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pairs = self.pairs[idx]\n",
    "        tokens1 = [pair[0] for pair in pairs]\n",
    "        tokens2 = [pair[1] for pair in pairs]\n",
    "        sentences1, sizes1 = utils._convert_pairs_to_indices(tokens1, self.word_dict,\n",
    "                                                             self.max_len1)\n",
    "        sentences2, sizes2 = utils._convert_pairs_to_indices(tokens2, self.word_dict,\n",
    "                                                             self.max_len2)\n",
    "        if self.label_dict is not None:\n",
    "            labels = utils.convert_labels(pairs, self.label_dict)\n",
    "        else:\n",
    "            labels = None\n",
    "        \n",
    "        return sentences1, sentences2, labels\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "class Utils:\n",
    "    def tokenize_english(self, text):\n",
    "        return tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "    def tokenize_corpus(self, pairs):\n",
    "        tokenized_pairs = []\n",
    "        for sent1, sent2, label in pairs:\n",
    "            tokens1 = self.tokenize_english(sent1)\n",
    "            tokens2 = self.tokenize_english(sent2)\n",
    "            tokenized_pairs.append((tokens1, tokens2, label))\n",
    "\n",
    "        return tokenized_pairs\n",
    "\n",
    "    def count_corpus_tokens(self, pairs):\n",
    "        \"\"\"\n",
    "        Examine all pairs ans extracts all tokens from both text and hypothesis.\n",
    "        :param pairs: a list of tuples (sent1, sent2, relation) with tokenized\n",
    "            sentences\n",
    "        :return: a Counter of lowercase tokens\n",
    "        \"\"\"\n",
    "        c = Counter()\n",
    "        for sent1, sent2, _ in pairs:\n",
    "            c.update(t.lower() for t in sent1)\n",
    "            c.update(t.lower() for t in sent2)\n",
    "\n",
    "        return c\n",
    "\n",
    "    def shuffle_arrays(self, *arrays):\n",
    "        rng_state = np.random.get_state()\n",
    "        for array in arrays:\n",
    "            np.random.shuffle(array)\n",
    "            np.random.set_state(rng_state)\n",
    "\n",
    "    def create_label_dict(self, pairs):\n",
    "        \"\"\"\n",
    "        Return a dictionary mapping the labels found in `pairs` to numbers\n",
    "        :param pairs: a list of tuples (_, _, label), with label as a string\n",
    "        :return: a dict\n",
    "        \"\"\"\n",
    "        labels = set(pair[2] for pair in pairs)\n",
    "        mapping = zip(labels, range(len(labels)))\n",
    "        params.nr_classes = len(labels)\n",
    "        return dict(mapping)\n",
    "\n",
    "\n",
    "    def convert_labels(self, pairs, label_map):\n",
    "        \"\"\"\n",
    "        Return a numpy array representing the labels in `pairs`\n",
    "        :param pairs: a list of tuples (_, _, label), with label as a string\n",
    "        :param label_map: dictionary mapping label strings to numbers\n",
    "        :return: a numpy array\n",
    "        \"\"\"\n",
    "        return np.array([label_map[pair[2]] for pair in pairs], dtype=np.int32)\n",
    "\n",
    "\n",
    "    def create_dataset(self, pairs, word_dict, label_dict=None,\n",
    "                       max_len1=None, max_len2=None):\n",
    "        \"\"\"\n",
    "        Generate and return a RTEDataset object for storing the data in numpy format.\n",
    "        :param pairs: list of tokenized tuples (sent1, sent2, label)\n",
    "        :param word_dict: a dictionary mapping words to indices\n",
    "        :param label_dict: a dictionary mapping labels to numbers. If None,\n",
    "            labels are ignored.\n",
    "        :param max_len1: the maximum length that arrays for sentence 1\n",
    "            should have (i.e., time steps for an LSTM). If None, it\n",
    "            is computed from the data.\n",
    "        :param max_len2: same as max_len1 for sentence 2\n",
    "        :return: RTEDataset\n",
    "        \"\"\"\n",
    "        tokens1 = [pair[0] for pair in pairs]\n",
    "        tokens2 = [pair[1] for pair in pairs]\n",
    "        sentences1, sizes1 = self._convert_pairs_to_indices(tokens1, word_dict,\n",
    "                                                            max_len1)\n",
    "        sentences2, sizes2 = self._convert_pairs_to_indices(tokens2, word_dict,\n",
    "                                                            max_len2)\n",
    "        if label_dict is not None:\n",
    "            labels = self.convert_labels(pairs, label_dict)\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        return NLIDataset(sentences1, sentences2, sizes1, sizes2, labels)\n",
    "    \n",
    "    \n",
    "    def collate_batch(self, batch):\n",
    "        if params.use_optimizations:\n",
    "            premise = np.array([k for bat in batch for k in bat[0]], dtype=np.float)\n",
    "            hypo = np.array([k for bat in batch for k in bat[1]], dtype=np.float)\n",
    "            labels = np.array([k for bat in batch for k in bat[2]], dtype=np.float)\n",
    "            return torch.from_numpy(premise), torch.from_numpy(hypo), torch.from_numpy(labels)\n",
    "        else:\n",
    "            return default_collate(batch)\n",
    "\n",
    "\n",
    "    def _convert_pairs_to_indices(self, sentences, word_dict, max_len=None,\n",
    "                                  use_null=True):\n",
    "        sizes = np.array([len(sent) for sent in sentences])\n",
    "        if use_null:\n",
    "            sizes += 1\n",
    "            if max_len is not None:\n",
    "                max_len += 1\n",
    "\n",
    "        if max_len is None:\n",
    "            max_len = sizes.max()\n",
    "\n",
    "        shape = (len(sentences), max_len)\n",
    "        array = np.full(shape, word_dict[PADDING], dtype=np.int32)\n",
    "\n",
    "        for i, sent in enumerate(sentences):\n",
    "            words = []\n",
    "            \n",
    "            if len(sent) <= max_len - 1:\n",
    "                words = sent\n",
    "            else:\n",
    "                idx = 0\n",
    "                while len(words) < max_len - 1:\n",
    "                    words.append(sent[idx])\n",
    "                    idx += 1\n",
    "                \n",
    "            indices = [word_dict[token] for token in words]\n",
    "\n",
    "            if use_null:\n",
    "                indices = [word_dict[GO]] + indices\n",
    "\n",
    "            array[i, :len(indices)] = indices\n",
    "\n",
    "        return array, sizes\n",
    "\n",
    "\n",
    "    def load_parameters(self, dirname):\n",
    "        filename = os.path.join(dirname, 'model-params.json')\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def get_sentence_sizes(self, pairs):\n",
    "        sizes1 = np.array([len(pair[0]) for pair in pairs])\n",
    "        sizes2 = np.array([len(pair[1]) for pair in pairs])\n",
    "        return (sizes1, sizes2)\n",
    "\n",
    "\n",
    "    def get_max_sentence_sizes(self, pairs1, pairs2):\n",
    "        train_sizes1, train_sizes2 = self.get_sentence_sizes(pairs1)\n",
    "        valid_sizes1, valid_sizes2 = self.get_sentence_sizes(pairs2)\n",
    "        train_max1 = max(train_sizes1)\n",
    "        valid_max1 = max(valid_sizes1)\n",
    "        max_size1 = max(train_max1, valid_max1)\n",
    "        train_max2 = max(train_sizes2)\n",
    "        valid_max2 = max(valid_sizes2)\n",
    "        max_size2 = max(train_max2, valid_max2)\n",
    "\n",
    "        return max_size1, max_size2\n",
    "\n",
    "\n",
    "    def normalize_embeddings(self, embeddings):\n",
    "        \"\"\"\n",
    "        Normalize the embeddings to have norm 1.\n",
    "        :param embeddings: 2-d numpy array\n",
    "        :return: normalized embeddings\n",
    "        \"\"\"\n",
    "        # normalize embeddings\n",
    "        norms = np.linalg.norm(embeddings.numpy(), axis=1).reshape((-1, 1))\n",
    "        embeddings = torch.from_numpy(embeddings.numpy() / norms)\n",
    "        return embeddings\n",
    "    \n",
    "    def save_checkpoint(self, state, is_best, filename='checkpoint.pth.tar'):\n",
    "        torch.save(state, filename)\n",
    "        epoch = state['epoch']\n",
    "        print(\"=> Saving model to %s\" % filename)\n",
    "        if epoch % 50 == 0 and epoch != 0:\n",
    "            shutil.copyfile(filename, 'model_' + str(epoch) + '.pth.tar')\n",
    "        if is_best:\n",
    "            print(\"=> The model just saved has performed best on validation set\" +\n",
    "                     \" till now\")\n",
    "            shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "        \n",
    "        return filename\n",
    "\n",
    "\n",
    "    def load_checkpoint(self, resume):\n",
    "        if os.path.isfile(resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "            checkpoint = torch.load(resume)\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                     .format(resume, checkpoint['epoch']))\n",
    "            return checkpoint\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "            return None\n",
    "    \n",
    "    def get_time_hhmmss(self, start = None):\n",
    "        \"\"\"\n",
    "        Calculates time since `start` and formats as a string.\n",
    "        \"\"\"\n",
    "        if start is None:\n",
    "            return time.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "        end = time.time()\n",
    "        m, s = divmod(end - start, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        time_str = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "        return time_str  \n",
    "\n",
    "utils = Utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IO:\n",
    "    def _generate_random_vector(self, size):\n",
    "        \"\"\"\n",
    "        Generate a random vector from a uniform distribution between\n",
    "        -0.1 and 0.1.\n",
    "        \"\"\"\n",
    "        return np.random.uniform(-0.1, 0.1, size)\n",
    "\n",
    "    def write_extra_embeddings(self, embeddings, dirname):\n",
    "        \"\"\"\n",
    "        Write the extra embeddings (for unknown, padding and null)\n",
    "        to a numpy file. They are assumed to be the first three in\n",
    "        the embeddings model.\n",
    "        \"\"\"\n",
    "        path = os.path.join(dirname, 'extra-embeddings.npy')\n",
    "        torch.save(embeddings[:3], path)\n",
    "        \n",
    "    def load_embeddings(self, normalize=True, generate=True):\n",
    "        glove = vocab.GloVe(name='6B', dim=params.projection_dim)\n",
    "        wordlist, embeddings = glove.stoi, glove.vectors\n",
    "\n",
    "        mapping = zip(wordlist, range(3, len(wordlist) + 3))\n",
    "\n",
    "        # always map OOV words to 0\n",
    "        wd = defaultdict(int, mapping)\n",
    "        wd[UNKNOWN] = 0\n",
    "        wd[PADDING] = 1\n",
    "        wd[GO] = 2\n",
    "\n",
    "        if generate:\n",
    "            vector_size = embeddings.shape[1]\n",
    "            extra = torch.FloatTensor([\n",
    "                     self._generate_random_vector(vector_size),\n",
    "                     self._generate_random_vector(vector_size),\n",
    "                     self._generate_random_vector(vector_size)])\n",
    "            self.write_extra_embeddings(extra, \"/scratch/as10656/\")\n",
    "\n",
    "        else:\n",
    "            path = os.path.join(load_extra_from, 'extra-embeddings.npy')\n",
    "            extra = torch.load(path)\n",
    "\n",
    "        embeddings = torch.cat((extra, embeddings), 0)\n",
    "\n",
    "\n",
    "        print('Embeddings have shape {}'.format(embeddings.shape))\n",
    "        if normalize:\n",
    "            embeddings = utils.normalize_embeddings(embeddings)\n",
    "\n",
    "        nn_embedding = nn.Embedding(embeddings.shape[0],\n",
    "                                    embeddings.shape[1])\n",
    "\n",
    "        nn_embedding.weight.data.copy_(embeddings)\n",
    "\n",
    "        # Fix weights for training\n",
    "        nn_embedding.weight.requires_grad = False\n",
    "\n",
    "\n",
    "        return wd, nn_embedding\n",
    "\n",
    "    def read_corpus(self, filename, lowercase, language='en'):\n",
    "        print('Reading data from %s' % filename)\n",
    "        # we are only interested in the actual sentences + gold label\n",
    "        # the corpus files has a few more things\n",
    "        useful_data = []\n",
    "        # the Multinli corpus has one JSON object per line\n",
    "        with open(filename, 'rb') as f:\n",
    "            for line in f:\n",
    "                line = line.decode('utf-8')\n",
    "                if lowercase:\n",
    "                    line = line.lower()\n",
    "                data = json.loads(line)\n",
    "                if data['gold_label'] == '-':\n",
    "                    # ignore items without a gold label\n",
    "                    continue\n",
    "                    \n",
    "                sentence1_parse = data['sentence1_parse']\n",
    "                sentence2_parse = data['sentence2_parse']\n",
    "                label = data['gold_label']\n",
    "\n",
    "                tree1 = nltk.Tree.fromstring(sentence1_parse)\n",
    "                tree2 = nltk.Tree.fromstring(sentence2_parse)\n",
    "                tokens1 = tree1.leaves()\n",
    "                tokens2 = tree2.leaves()\n",
    "                t = (tokens1, tokens2, label)\n",
    "                useful_data.append(t)\n",
    "\n",
    "        return useful_data\n",
    "\n",
    "    def read_corpus_batched(self, filename, lowercase, language='en'):\n",
    "        print('Reading data from %s' % filename)\n",
    "        # we are only interested in the actual sentences + gold label\n",
    "        # the corpus files has a few more things\n",
    "        useful_data = []\n",
    "        done = dict()\n",
    "        # the Multinli corpus has one JSON object per line\n",
    "        with open(filename, 'rb') as f:\n",
    "            for line in f:\n",
    "                line = line.decode('utf-8')\n",
    "                if lowercase:\n",
    "                    line = line.lower()\n",
    "                data = json.loads(line)\n",
    "                if data['gold_label'] == '-':\n",
    "                    # ignore items without a gold label\n",
    "                    continue\n",
    "                prompt_id = data['promptid']\n",
    "                \n",
    "                if prompt_id not in done:\n",
    "                    done[prompt_id] = len(useful_data)\n",
    "                    useful_data.append([])\n",
    "                \n",
    "                sentence1_parse = data['sentence1_parse']\n",
    "                sentence2_parse = data['sentence2_parse']\n",
    "                label = data['gold_label']\n",
    "\n",
    "                tree1 = nltk.Tree.fromstring(sentence1_parse)\n",
    "                tree2 = nltk.Tree.fromstring(sentence2_parse)\n",
    "                tokens1 = tree1.leaves()\n",
    "                tokens2 = tree2.leaves()\n",
    "                t = (tokens1, tokens2, label)\n",
    "                useful_data[done[prompt_id]].append(t)\n",
    "\n",
    "        return useful_data\n",
    "    \n",
    "io = IO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    \"\"\"\n",
    "    Provides early stopping functionality. Keeps track of model accuracy, \n",
    "    and if it doesn't improve over time restores last best performing \n",
    "    parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, patience = 100, minimize = True):\n",
    "        \"\"\"\n",
    "        Initialises a `EarlyStopping` isntance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        saver     : \n",
    "                    TensorFlow Saver object to be used for saving and restoring model.\n",
    "        session   : \n",
    "                    TensorFlow Session object containing graph where model is restored.\n",
    "        patience  : \n",
    "                    Early stopping patience. This is the number of epochs we wait for \n",
    "                    accuracy to start improving again before stopping and restoring \n",
    "                    previous best performing parameters.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        New instance.\n",
    "        \"\"\"\n",
    "        self.minimize = minimize\n",
    "        self.patience = patience\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.best_monitored_value = 0. if minimize else np.inf\n",
    "        self.best_monitored_acc = np.inf if minimize else 0.\n",
    "        self.best_monitored_epoch = 0\n",
    "\n",
    "        self.restore_path = None\n",
    "\n",
    "    def __call__(self, value, acc, epoch, rest):\n",
    "        \"\"\"\n",
    "        Checks if we need to stop and restores the last well performing values if we do.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        value     : \n",
    "                    Last epoch monitored value.\n",
    "        acc       :\n",
    "                    Current accuracy\n",
    "        epoch     : \n",
    "                    Last epoch number.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        `True` if we waited enough and it's time to stop and we restored the \n",
    "        best performing weights, or `False` otherwise.\n",
    "        \"\"\"\n",
    "        if (self.minimize and acc < self.best_monitored_acc) or (not self.minimize and acc > self.best_monitored_acc):\n",
    "            self.best_monitored_value = value\n",
    "            self.best_monitored_epoch = epoch\n",
    "            self.best_monitored_acc = acc\n",
    "            state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': self.model.state_dict(),\n",
    "                'best': value,\n",
    "                'best_acc': acc,\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "            }            \n",
    "            \n",
    "            rest.update(state)\n",
    "            self.restore_path = utils.save_checkpoint(rest, True, \"/scratch/as10656/nli_models/early_stopping_checkpoint\")\n",
    "        elif self.best_monitored_epoch + self.patience < epoch:\n",
    "            if self.restore_path != None:\n",
    "                checkpoint = utils.load_checkpoint(self.restore_path)\n",
    "                self.best_monitored_value = checkpoint['best']\n",
    "                self.best_monitored_acc = checkpoint['best_acc']\n",
    "                self.best_monitored_epoch = checkpoint['epoch']\n",
    "                self.model.load_state_dict(checkpoint['state_dict'])\n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            else:\n",
    "                print(\"ERROR: Failed to restore session\")\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def init_from_checkpoint(self, checkpoint):\n",
    "        self.best_monitored_value = checkpoint['best']\n",
    "        self.best_monitored_acc = checkpoint['best_acc']\n",
    "        self.best_monitored_epoch = 0\n",
    "    \n",
    "    def print_info(self):\n",
    "        print(\"Best loss: {0}, Best Accuracy: {1}, at epoch {2}\"\n",
    "              .format(self.best_monitored_value,\n",
    "                      self.best_monitored_acc,\n",
    "                      self.best_monitored_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_curve(axis, params, train_column, valid_column, linewidth = 2, train_linestyle = \"b-\", valid_linestyle = \"g-\"):\n",
    "    \"\"\"\n",
    "    Plots a pair of validation and training curves on a single plot.\n",
    "    \"\"\"\n",
    "    train_values = params[train_column]\n",
    "    valid_values = params[valid_column]\n",
    "    epochs = train_values.shape[0]\n",
    "    x_axis = np.arange(epochs)\n",
    "    axis.plot(x_axis[train_values > 0], train_values[train_values > 0], train_linestyle, linewidth=linewidth, label=\"train\")\n",
    "    axis.plot(x_axis[valid_values > 0], valid_values[valid_values > 0], valid_linestyle, linewidth=linewidth, label=\"valid\")\n",
    "    return epochs\n",
    "\n",
    "# Plots history of learning curves for a specific model.\n",
    "def plot_learning_curves(params):\n",
    "    \"\"\"\n",
    "    Plots learning curves (loss and accuracy on both training and validation sets) for a model identified by a parameters struct.\n",
    "    \"\"\"\n",
    "    curves_figure = plt.figure(figsize = (10, 4))\n",
    "    axis = curves_figure.add_subplot(1, 2, 1)\n",
    "    epochs_plotted = plot_curve(axis, params, train_column = \"train_acc\", valid_column = \"val_acc\")\n",
    "\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.ylim(45., 115.)\n",
    "    plt.xlim(0, epochs_plotted)\n",
    "\n",
    "    axis = curves_figure.add_subplot(1, 2, 2)\n",
    "    epochs_plotted = plot_curve(axis, params, train_column = \"train_loss\", valid_column = \"val_loss\")\n",
    "\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.ylim(1, 50)\n",
    "    plt.xlim(0, epochs_plotted)\n",
    "#     plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Taken from allennlp\n",
    "class TimeDistributed(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Given an input shaped like ``(batch_size, time_steps, [rest])``\n",
    "    and a ``Module`` that takes inputs like ``(batch_size, [rest])``,\n",
    "    ``TimeDistributed`` reshapes the input to be\n",
    "    ``(batch_size * time_steps, [rest])``, applies the contained ``Module``,\n",
    "    then reshapes it back.\n",
    "    Note that while the above gives shapes with ``batch_size`` first,\n",
    "    this ``Module`` also works if ``batch_size`` is second -\n",
    "    we always just combine the first two dimensions, then split them.\n",
    "    \"\"\"\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self._module = module\n",
    "\n",
    "    def forward(self, *inputs):  # pylint: disable=arguments-differ\n",
    "        reshaped_inputs = []\n",
    "        for input_tensor in inputs:\n",
    "            input_size = input_tensor.size()\n",
    "            if len(input_size) <= 2:\n",
    "                raise RuntimeError(\"No dimension to distribute: \" +\n",
    "                                   str(input_size))\n",
    "\n",
    "            # Squash batch_size and time_steps into a single axis;\n",
    "            # result has shape (batch_size * time_steps, input_size).\n",
    "            squashed_shape = [-1] + [x for x in input_size[2:]]\n",
    "            reshaped_inputs.append(\n",
    "                input_tensor.contiguous().view(*squashed_shape))\n",
    "\n",
    "        reshaped_outputs = self._module(*reshaped_inputs)\n",
    "\n",
    "        # Now get the output back into the right shape.\n",
    "        # (batch_size, time_steps, [hidden_size])\n",
    "        new_shape = [input_size[0], input_size[1]] + \\\n",
    "                    [x for x in reshaped_outputs.size()[1:]]\n",
    "        outputs = reshaped_outputs.contiguous().view(*new_shape)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(model):\n",
    "    # As mentioned in paper\n",
    "    mean = 0\n",
    "    stddev = 0.01\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean, stddev)\n",
    "\n",
    "\n",
    "def get_embedded_mask(embedded):\n",
    "    return (embedded != 1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecomposableAttention(nn.Module):\n",
    "    def __init__(self, settings, embeddings):\n",
    "        super(DecomposableAttention, self).__init__()\n",
    "\n",
    "        if isinstance(settings, argparse.Namespace):\n",
    "            settings = vars(settings)\n",
    "            \n",
    "        self.embedding = embeddings\n",
    "        self.settings = settings\n",
    "        self.max_length = settings['max_len']\n",
    "        self.hidden_dim = settings['hidden_dim']\n",
    "        self.nr_classes = settings['nr_classes']\n",
    "        \n",
    "        if settings['hidden_dim'] != settings['projection_dim']:\n",
    "            self.projection = nn.Linear(self.hidden_dim, settings['projection_dim'])\n",
    "        else:\n",
    "            self.projection = lambda x: x\n",
    "            \n",
    "        settings['hidden_dim'] = settings['projection_dim']\n",
    "        self.hidden_dim = settings['hidden_dim']\n",
    "\n",
    "        if settings['gru_encode']:\n",
    "            self.encoder = BiRNNEncoder(self.max_length, self.hidden_dim,\n",
    "                                        dropout=settings['dropout'])\n",
    "        if settings['use_intra_attention']:\n",
    "            self.intra_sentence_attender = Attention(self.max_length,\n",
    "                                                     self.hidden_dim,\n",
    "                                                     dropout=settings['dropout'])\n",
    "\n",
    "\n",
    "            self.intra_align = SoftAlignment(self.max_length, self.hidden_dim)\n",
    "            self.intra_align_project = nn.Linear(self.hidden_dim * 2,\n",
    "                                                 self.hidden_dim)\n",
    "\n",
    "        self.attender = Attention(self.max_length, self.hidden_dim,\n",
    "                                  dropout=settings['dropout'])\n",
    "\n",
    "        self.align = SoftAlignment(self.max_length, self.hidden_dim)\n",
    "\n",
    "        self.compare = Comparison(self.max_length, self.hidden_dim,\n",
    "                                  dropout=settings['dropout'])\n",
    "        self.aggregate = Aggregate(self.hidden_dim, self.nr_classes,\n",
    "                                   dropout=settings['dropout'])\n",
    "\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, premise, hypo):\n",
    "        premise_mask = get_embedded_mask(premise)\n",
    "        hypo_mask = get_embedded_mask(hypo)\n",
    "        \n",
    "        premise = self.embedding(premise)\n",
    "        hypo = self.embedding(hypo)\n",
    "\n",
    "        premise = self.projection(premise)\n",
    "        hypo = self.projection(hypo)\n",
    "\n",
    "\n",
    "        if self.settings['gru_encode']:\n",
    "            premise = self.encoder(premise)\n",
    "            hypo = self.encoder(hypo)\n",
    "\n",
    "        if self.settings['use_intra_attention']:\n",
    "            # Intra Sentence Attention\n",
    "            premise = self.intra_attention(premise, premise_mask)\n",
    "            hypo = self.intra_attention(hypo, hypo_mask)\n",
    "\n",
    "            premise = self.intra_align_project(premise)\n",
    "            hypo = self.intra_align_project(hypo)\n",
    "\n",
    "        projected_premise = self.attender(premise)\n",
    "        projected_hypo = self.attender(hypo)\n",
    "\n",
    "        att_ji = projected_hypo.bmm(projected_premise.permute(0, 2, 1))\n",
    "\n",
    "        # Shape: batch_length * max_length * max_length\n",
    "        att_ij = att_ji.permute(0, 2, 1)\n",
    "\n",
    "        aligned_hypo = self.align(hypo, hypo_mask, att_ij)\n",
    "        aligned_premise = self.align(premise, premise_mask, att_ij, transpose=True)\n",
    "        \n",
    "        premise_compare_input = torch.cat([premise, aligned_hypo], dim=-1)\n",
    "        hypo_compare_input = torch.cat([hypo, aligned_premise], dim=-1)\n",
    "\n",
    "        compared_premise = self.compare(premise_compare_input)\n",
    "        compared_premise = compared_premise * premise_mask.unsqueeze(-1)\n",
    "        # Shape: (batch_size, compare_dim)\n",
    "        compared_premise = compared_premise.sum(dim=1)\n",
    "\n",
    "        compared_hypo = self.compare(hypo_compare_input)\n",
    "        compared_hypo = compared_hypo * hypo_mask.unsqueeze(-1)\n",
    "        # Shape: (batch_size, compare_dim)\n",
    "        compared_hypo = compared_hypo.sum(dim=1)\n",
    "\n",
    "        aggregate_input = torch.cat([compared_premise, compared_hypo], dim=-1)\n",
    "        scores = self.aggregate(aggregate_input)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def intra_attention(self, sentence, sentence_mask):\n",
    "        projected_sentence = self.intra_sentence_attender(sentence)\n",
    "        intra_att_ji = projected_sentence.bmm(projected_sentence.permute(0, 2,\n",
    "                                                                         1))\n",
    "        intra_att_ij = intra_att_ji.permute(0, 2, 1)\n",
    "        aligned_sentence = self.intra_align(sentence, sentence_mask, intra_att_ij)\n",
    "        sentence = torch.cat([sentence, aligned_sentence], dim=-1)\n",
    "\n",
    "        return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiRNNEncoder(nn.Module):\n",
    "    def __init__(self, max_length, nr_hidden, dropout=0.0):\n",
    "        super(BiRNNEncoder, self).__init__()\n",
    "\n",
    "        self.nr_hidden = nr_hidden\n",
    "\n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.Linear(nr_hidden * 2, nr_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(nr_hidden, nr_hidden, max_length,\n",
    "                            dropout=dropout,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        self.fully_connected = TimeDistributed(self.fully_connected)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output, _ = self.lstm(input)\n",
    "        return self.dropout(self.fully_connected(output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, max_length, nr_hidden, dropout=0.0):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.model = TimeDistributed(nn.Sequential(\n",
    "            nn.Linear(nr_hidden, nr_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(nr_hidden, nr_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout)\n",
    "        ))\n",
    "\n",
    "        init_weights(self.model)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        return self.model(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftAlignment(nn.Module):\n",
    "    def __init__(self, max_length, nr_hidden):\n",
    "        super(SoftAlignment, self).__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.nr_hidden = nr_hidden\n",
    "\n",
    "    def forward(self, sentence, mask, attention_matrix, transpose=False):\n",
    "        if transpose:\n",
    "            attention_matrix = attention_matrix.permute(0, 2, 1)\n",
    "\n",
    "        exponents = torch.exp(attention_matrix -\n",
    "                              torch.max(attention_matrix,\n",
    "                                        dim=-1,\n",
    "                                        keepdim=True)[0])\n",
    "        \n",
    "        exponents = exponents\n",
    "\n",
    "        summation = torch.sum(exponents, dim=-1, keepdim=True)\n",
    "\n",
    "        attention_weights = exponents / summation\n",
    "        \n",
    "        attention_weights = attention_weights * mask.unsqueeze(-1)\n",
    "        \n",
    "        attention_weights = attention_weights / (attention_weights.sum(-1, keepdim=True) + 1e-13) \n",
    "\n",
    "        return attention_weights.bmm(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Comparison(nn.Module):\n",
    "    def __init__(self, max_length, nr_hidden, dropout=0):\n",
    "        super(Comparison, self).__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.nr_hidden = nr_hidden\n",
    "        self.model = TimeDistributed(nn.Sequential(\n",
    "            nn.Linear(nr_hidden * 2, nr_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(nr_hidden, nr_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout)\n",
    "        ))\n",
    "        init_weights(self.model)\n",
    "\n",
    "    def forward(self, concatenated_aligned_sentence):\n",
    "        return self.model(concatenated_aligned_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Aggregate(nn.Module):\n",
    "    def __init__(self, nr_hidden, nr_out, dropout=0.0):\n",
    "        super(Aggregate, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(nr_hidden * 2, nr_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(nr_hidden, nr_out)\n",
    "        )\n",
    "\n",
    "        init_weights(self.model)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiLSTMMaxPooling(nn.Module):\n",
    "    def __init__(self, params, embedding):\n",
    "        super(BiLSTMMaxPooling, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding.embedding_dim\n",
    "\n",
    "        self.max_length = params.max_len\n",
    "        self.nr_hidden = params.hidden_dim\n",
    "        self.nr_classes = params.nr_classes\n",
    "        \n",
    "        self.bilstm = nn.LSTM(self.embedding_dim, self.nr_hidden, \n",
    "                              num_layers=1, \n",
    "                              batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=params.dropout)\n",
    "        \n",
    "        self.linear_1 = nn.Linear(8 * self.nr_hidden, self.nr_hidden)\n",
    "        self.linear_2 = nn.Linear(9 * self.nr_hidden, self.nr_hidden)\n",
    "        self.linear_3 = nn.Linear(self.nr_hidden, self.nr_classes)\n",
    "        \n",
    "        # Glove embedding\n",
    "        self.embedding = embedding\n",
    "        \n",
    "    def forward(self, premise, hypo):\n",
    "        premise_mask = get_embedded_mask(premise)\n",
    "        hypo_mask = get_embedded_mask(hypo)\n",
    "        \n",
    "        # B x T => B x T x E\n",
    "        premise = self.embedding(premise)\n",
    "        hypo = self.embedding(hypo)\n",
    "        \n",
    "        # Encode with bilstm and take all of the hidden states for each t belonging to seq_len\n",
    "        # B x T x E => B x T x 2H\n",
    "        premise_output, _ = self.bilstm(premise)\n",
    "        \n",
    "        # Take max pool along seq_len\n",
    "        # B x T x 2H => B x 2H\n",
    "        premise_encoded, _ = premise_output.max(dim=1)\n",
    "        \n",
    "        premise_encoded = self.dropout(premise_encoded)\n",
    "        \n",
    "        hypo_output, _ = self.bilstm(hypo)\n",
    "        hypo_encoded, _ = hypo_output.max(dim=1)\n",
    "        \n",
    "        hypo_encoded = self.dropout(hypo_encoded)\n",
    "        \n",
    "        # B x 2H => B x 8H\n",
    "        lin_input = torch.cat([premise_encoded, hypo_encoded, \n",
    "                               premise_encoded - hypo_encoded, \n",
    "                               torch.mul(premise_encoded, hypo_encoded)], \n",
    "                              dim=1)\n",
    "        \n",
    "        # B x 8H => B x H\n",
    "        lin_output = self.linear_1(lin_input)\n",
    "        lin_output = nn.functional.relu(lin_output)\n",
    "        \n",
    "        lin_output = self.dropout(lin_output)\n",
    "        \n",
    "        # Skip connections, B x H => B x 9H\n",
    "        lin_input_2 = torch.cat([lin_input, lin_output], dim=1)\n",
    "        \n",
    "        # B x 9H => B x H\n",
    "        lin_output_2 = self.linear_2(lin_input_2)\n",
    "        lin_output_2 = nn.functional.relu(lin_output_2)\n",
    "\n",
    "        lin_output = self.dropout(lin_output_2)\n",
    "        \n",
    "        # B x H => B x 3\n",
    "        return self.linear_3(lin_output_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, params, train_data, dev_data, dev_mismatched_data, embedding):\n",
    "        self.params = params\n",
    "        self.train_data = train_data\n",
    "        self.dev_data = dev_data\n",
    "        self.dev_mismatched_data = dev_mismatched_data\n",
    "        self.epochs = params.epochs\n",
    "        print(\"Creating dataloaders\")\n",
    "        self.cuda_available = torch.cuda.is_available()\n",
    "        \n",
    "        self.train_loader = DataLoader(dataset=train_data,\n",
    "                                       shuffle=True,\n",
    "                                       batch_size=params.batch_size,\n",
    "                                       pin_memory=self.cuda_available,\n",
    "                                       collate_fn=utils.collate_batch)\n",
    "        self.dev_loader = DataLoader(dataset=dev_data,\n",
    "                                     shuffle=False,\n",
    "                                     batch_size=params.batch_size,\n",
    "                                     pin_memory=self.cuda_available)\n",
    "        self.dev_mismatched_loader = DataLoader(dataset=dev_mismatched_data,\n",
    "                                                shuffle=False,\n",
    "                                                batch_size=params.batch_size,\n",
    "                                                pin_memory=self.cuda_available)\n",
    "        \n",
    "        self.string_fixer = \"==========\"\n",
    "        self.embedding = embedding\n",
    "        self.writer = SummaryWriter(\"/scratch/as10656/nli_models/logs/opti\")\n",
    "        self.writer_step = 0\n",
    "\n",
    "    \n",
    "    def load(self, model_name=\"decomposable\"):\n",
    "        print(\"Loading model\")\n",
    "        \n",
    "        if model_name == \"decomposable\":\n",
    "            self.model = DecomposableAttention(self.params, self.embedding)\n",
    "        else:\n",
    "            self.model = BiLSTMMaxPooling(self.params, self.embedding)\n",
    "        self.optimizer = optim.Adam(filter(lambda p: p.requires_grad,\n",
    "                                           self.model.parameters()),\n",
    "                               lr=params.lr)\n",
    "\n",
    "        self.start_time = time.time()\n",
    "        self.histories = {\n",
    "            \"train_loss\": np.empty(0, dtype=np.float32),\n",
    "            \"train_acc\": np.empty(0, dtype=np.float32),\n",
    "            \"dev_matched_loss\": np.empty(0, dtype=np.float32),\n",
    "            \"dev_matched_acc\": np.empty(0, dtype=np.float32),\n",
    "            \"dev_mismatched_loss\": np.empty(0, dtype=np.float32),\n",
    "            \"dev_mismatched_acc\": np.empty(0, dtype=np.float32)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        self.early_stopping = EarlyStopping(self.model, self.optimizer, patience=self.params.patience, minimize=False)\n",
    "        if self.params.resume:\n",
    "            checkpoint = utils.load_checkpoint(self.params.resume)\n",
    "            if checkpoint is not None:\n",
    "                self.model.load_state_dict(checkpoint['state_dict'])\n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "                self.histories.update(checkpoint)\n",
    "                self.early_stopping.init_from_checkpoint(checkpoint)\n",
    "                print(\"Loaded model, Best Loss: %.8f, Best Acc: %.2f\" % (checkpoint['best'], checkpoint['best_acc']))\n",
    "\n",
    "        if self.cuda_available:\n",
    "            self.model = self.model.cuda()\n",
    "        \n",
    "        print(\"Model loaded\")\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        start_epoch = 0\n",
    "        best_prec = 0\n",
    "        \n",
    "        is_best = False\n",
    "\n",
    "        self.model.train()\n",
    "        print(\"Starting training\")\n",
    "        self.print_info()\n",
    "        for epoch in range(start_epoch, self.params.epochs):\n",
    "            for i, (premise, hypo, labels) in enumerate(self.train_loader):\n",
    "                premise_batch = Variable(premise.long())\n",
    "                hypo_batch = Variable(hypo.long())\n",
    "                labels_batch = Variable(labels)\n",
    "                if self.cuda_available:\n",
    "                    premise_batch = premise_batch.cuda()\n",
    "                    hypo_batch = hypo_batch.cuda()\n",
    "                    labels_batch = labels_batch.cuda(async=True)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(premise_batch, hypo_batch)\n",
    "                loss = criterion(output, labels_batch.long())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if self.params.extra_debug and (i + 1) % (self.params.batch_size * 4) == 0:\n",
    "                    for n, p in filter(lambda np: np[1].grad is not None, self.model.named_parameters()):\n",
    "                        print(self.writer_step)\n",
    "                        self.writer.add_histogram(n, p.grad.data.cpu().numpy(), global_step=self.writer_step)\n",
    "                    self.writer_step += 1\n",
    "                    \n",
    "                    if self.writer_step == 1200:\n",
    "                        break\n",
    "                    print(('Epoch: [{0}/{1}], Step: [{2}/{3}], Loss: {4},')\n",
    "                             .format(epoch + 1,\n",
    "                                     self.params.epochs,\n",
    "                                     i + 1,\n",
    "                                     len(self.train_loader),\n",
    "                                     loss.data[0]))\n",
    "            train_acc, train_loss = self.validate_model(self.train_loader, self.model)\n",
    "            dev_matched_acc, dev_matched_loss = self.validate_model(self.dev_loader, self.model)\n",
    "            dev_mismatched_acc, dev_mismatched_loss = self.validate_model(self.dev_mismatched_loader, self.model)\n",
    "            \n",
    "            self.histories['train_loss'] = np.append(self.histories['train_loss'], [train_loss])\n",
    "            self.histories['train_acc'] = np.append(self.histories['train_acc'], [train_acc])\n",
    "            self.histories['dev_matched_loss'] = np.append(self.histories['dev_matched_loss'], [dev_matched_loss])\n",
    "            self.histories['dev_matched_acc'] = np.append(self.histories['dev_matched_acc'], [dev_matched_acc])\n",
    "            self.histories['dev_mismatched_loss'] = np.append(self.histories['dev_mismatched_loss'], [dev_mismatched_loss])\n",
    "            self.histories['dev_mismatched_acc'] = np.append(self.histories['dev_mismatched_acc'], [dev_mismatched_acc])\n",
    "            \n",
    "            if not self.early_stopping(dev_matched_loss, dev_matched_acc, epoch, self.histories):\n",
    "                self.print_train_info(epoch, train_acc, train_loss, dev_matched_acc, dev_matched_loss, dev_mismatched_acc, dev_mismatched_loss)\n",
    "            else:\n",
    "                print(\"Early stopping activated\")\n",
    "                print(\"Restoring earlier state and stopping\")\n",
    "                self.early_stopping.print_info()\n",
    "                plot_learning_curves(self.histories)\n",
    "                plt.show()\n",
    "                break\n",
    "            \n",
    "\n",
    "            \n",
    "    def validate_model(self, loader, model):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        for premise, hypo, labels in loader:\n",
    "            premise_batch = Variable(premise.long(), volatile=True)\n",
    "            hypo_batch = Variable(hypo.long(), volatile=True)\n",
    "            labels_batch = Variable(labels.long())\n",
    "\n",
    "            if self.cuda_available:\n",
    "                premise_batch = premise_batch.cuda()\n",
    "                hypo_batch = hypo_batch.cuda()\n",
    "                labels_batch = labels_batch.cuda()\n",
    "\n",
    "            output = model(premise_batch, hypo_batch)\n",
    "            loss = nn.functional.cross_entropy(output, labels_batch.long(), size_average=False)\n",
    "            total_loss += loss.data[0]\n",
    "            total += len(labels_batch)\n",
    "            \n",
    "            if not self.cuda_available:\n",
    "                correct += (labels_batch == output.max(1)[1]).data.cpu().numpy().sum()\n",
    "            else:\n",
    "                correct += (labels_batch == output.max(1)[1]).data.sum()\n",
    "                \n",
    "        model.train()\n",
    "\n",
    "        average_loss = total_loss / total\n",
    "        return correct / total * 100, average_loss\n",
    "\n",
    "    def print_info(self):\n",
    "        print(self.string_fixer + \" Data \" + self.string_fixer)\n",
    "        print(\"Training set: %d examples\" % (len(self.train_data)))\n",
    "        print(\"Validation set: %d examples\" % (len(self.dev_data)))\n",
    "        print(\"Timestamp: %s\" % utils.get_time_hhmmss())\n",
    "        \n",
    "        print(self.string_fixer + \" Params \" + self.string_fixer)\n",
    "    \n",
    "        print(\"Learning Rate: %f\" % self.params.lr)\n",
    "        print(\"Dropout (p): %f\" % self.params.dropout)\n",
    "        print(\"Batch Size: %d\" % self.params.batch_size)\n",
    "        print(\"Epochs: %d\" % self.params.epochs)\n",
    "        print(\"Patience: %d\" % self.params.patience)\n",
    "        print(\"Resume: %s\" % self.params.resume)\n",
    "        print(\"GRU Encode: %s\" % str(self.params.gru_encode))\n",
    "        print(\"Cuda: %s\" % str(torch.cuda.is_available()))\n",
    "        print(\"Batch Optimizations: %s\" % str(self.params.use_optimizations))\n",
    "        print(\"Intra Attention: %s\" % str(self.params.use_intra_attention))\n",
    "        print(\"Model Structure:\")\n",
    "        print(self.model)\n",
    "        \n",
    "    def print_train_info(self, epoch, train_acc, train_loss, dev_acc, dev_loss, dev_mismatched_acc, dev_mismatched_loss):\n",
    "        print((self.string_fixer + \" Epoch: {0}/{1} \" + self.string_fixer)\n",
    "              .format(epoch + 1, self.params.epochs))\n",
    "        print(\"Train Loss: %.8f, Train Acc: %.2f\" % (train_loss, train_acc))\n",
    "        print(\"Dev Matched Loss: %.8f, Dev Matched Acc: %.2f\" % (dev_loss, dev_acc))\n",
    "        print(\"Dev Mismatched Loss: %.8f, Dev Mismatched Acc: %.2f\" % (dev_mismatched_loss, dev_mismatched_acc))\n",
    "        self.early_stopping.print_info()\n",
    "        print(\"Elapsed Time: %s\" % (utils.get_time_hhmmss(self.start_time)))\n",
    "        print(\"Current timestamp: %s\" % (utils.get_time_hhmmss()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(params, model_name=\"decomposable\", load_only=False):\n",
    "    wd, embeddings = io.load_embeddings()\n",
    "\n",
    "    dev_pairs = io.read_corpus(params.dev_file, True)\n",
    "    label_dict = utils.create_label_dict(dev_pairs)\n",
    "    dev_data = utils.create_dataset(dev_pairs, wd, label_dict, max_len1=params.max_len, max_len2=params.max_len)\n",
    "\n",
    "    dev_mismatched_pairs = io.read_corpus(params.dev_mistmatched_file, True)\n",
    "    dev_mismatched_data = utils.create_dataset(dev_mismatched_pairs, wd, label_dict, max_len1=params.max_len, max_len2=params.max_len) \n",
    "    if params.use_optimizations:\n",
    "        train_pairs = io.read_corpus_batched(params.train_file, True)\n",
    "    #     train_pairs = [pair for pairs in train_pairs for pair in pairs]\n",
    "        train_data = BatchedNLIDataset(train_pairs, wd, params.max_len, params.max_len, label_dict)\n",
    "    else:\n",
    "        train_pairs = io.read_corpus(params.train_file, True)\n",
    "        train_data = utils.create_dataset(train_pairs, wd, label_dict, max_len1=params.max_len, max_len2=params.max_len)\n",
    "\n",
    "\n",
    "    del train_pairs\n",
    "    del dev_pairs\n",
    "    del dev_mismatched_pairs\n",
    "\n",
    "\n",
    "    trainer = Trainer(params, train_data, dev_data, dev_mismatched_data, embeddings)\n",
    "    trainer.load(model_name)\n",
    "    \n",
    "    if not load_only:\n",
    "        trainer.train()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test without batch optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params.resume = False\n",
    "params.use_intra_attention = True\n",
    "params.gru_encode = False\n",
    "# Change to 32 for testing withouth optimization\n",
    "params.batch_size = 32\n",
    "params.projection_dim = 200\n",
    "params.use_optimization = False\n",
    "params.patience = 30\n",
    "\n",
    "train(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with batch optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params.resume = False\n",
    "params.use_intra_attention = True\n",
    "params.gru_encode = False\n",
    "params.batch_size = 10\n",
    "params.use_optimization = True\n",
    "params.projection_dim = 200\n",
    "params.patience = 30\n",
    "\n",
    "train(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test without batch optimizations for bilstm max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.projection_dim = 300\n",
    "params.hidden_dim = 300\n",
    "params.batch_size = 10\n",
    "params.dropout = 0.2\n",
    "params.resume = None\n",
    "params.extra_debug = True\n",
    "params.use_optimizations = True\n",
    "train(params, model_name=\"bilstm_max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp /scratch/as10656/nli_models/early_stopping_checkpoint /scratch/as10656/nli_models/bilstm_max_drop_0.5.pth.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with optimizations for bilstm max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params.projection_dim = 300\n",
    "params.batch_size = 10\n",
    "params.dropout = 0.5\n",
    "params.resume = False\n",
    "# params.resume = '/scratch/as10656/nli_models/early_stopping_checkpoint'\n",
    "params.use_optimizations = True\n",
    "train(params, model_name=\"bilstm_max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp /scratch/as10656/nli_models/early_stopping_checkpoint /scratch/as10656/nli_models/bilstm_max_drop_opti_0.5.pth.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params.projection_dim = 300\n",
    "params.batch_size = 10\n",
    "params.dropout = 0.2\n",
    "params.use_optimizations = True\n",
    "train(params, model_name=\"bilstm_max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
