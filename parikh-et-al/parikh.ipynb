{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext.vocab as vocab\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import argparse\n",
    "import jsonlines\n",
    "import ujson as json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import shutil\n",
    "from collections import Counter, defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "UNKNOWN = '**UNK**'\n",
    "PADDING = '**PAD**'\n",
    "GO = '**GO**'  # it's called \"GO\" but actually serves as a null alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = argparse.Namespace()\n",
    "params.projection_dim = 200\n",
    "params.batch_size = 32\n",
    "params.max_len = 100\n",
    "params.hidden_dim = params.projection_dim\n",
    "params.train_file = \"/home/as10656/nli-batch-optimizations/multinli_1.0/multinli_1.0_train.jsonl\"\n",
    "params.train_small_file = \"/home/as10656/nli-batch-optimizations/multinli_1.0/multinli_1.0_train_small.jsonl\"\n",
    "params.dev_file = \"/home/as10656/nli-batch-optimizations/multinli_1.0/multinli_1.0_dev_matched.jsonl\"\n",
    "params.dev_mistmatched_file = \"/home/as10656/nli-batch-optimizations/multinli_1.0/multinli_1.0_dev_mismatched.jsonl\"\n",
    "params.lr = 0.0001\n",
    "params.seed = 7\n",
    "params.gru_encode = False\n",
    "params.dropout = 0.2\n",
    "params.nr_classes = 3\n",
    "params.epochs = 300\n",
    "params.patience = 20\n",
    "params.resume = False\n",
    "params.extra_debug = False\n",
    "params.use_optimizations = False\n",
    "params.use_intra_attention = True\n",
    "torch.manual_seed(params.seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modified version of utils and ioutils classes from https://github.com/erickrf/multiffn-nli\n",
    "class NLIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sentences1, sentences2, sizes1, sizes2, labels):\n",
    "        \"\"\"\n",
    "        :param sentences1: A 2D numpy array with sentences (the first in each\n",
    "            pair) composed of token indices\n",
    "        :param sentences2: Same as above for the second sentence in each pair\n",
    "        :param sizes1: A 1D numpy array with the size of each sentence in the\n",
    "            first group. Sentences should be filled with the PADDING token after\n",
    "            that point\n",
    "        :param sizes2: Same as above\n",
    "        :param labels: 1D numpy array with labels as integers\n",
    "        \"\"\"\n",
    "        self.sentences1 = sentences1\n",
    "        self.sentences2 = sentences2\n",
    "        self.sizes1 = sizes1\n",
    "        self.sizes2 = sizes2\n",
    "        self.labels = labels\n",
    "        self.num_items = len(sentences1)\n",
    "\n",
    "    def shuffle_data(self):\n",
    "        \"\"\"\n",
    "        Shuffle all data using the same random sequence.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        shuffle_arrays(self.sentences1, self.sentences2,\n",
    "                       self.sizes1, self.sizes2, self.labels)\n",
    "\n",
    "    def get_batch(self, from_, to):\n",
    "        \"\"\"\n",
    "        Return an NLIDataset object with the subset of the data contained in\n",
    "        the given interval. Note that the actual number of items may be less\n",
    "        than (`to` - `from_`) if there are not enough of them.\n",
    "        :param from_: which position to start from\n",
    "        :param to: which position to end\n",
    "        :return: an NLIDataset object\n",
    "        \"\"\"\n",
    "        if from_ == 0 and to >= self.num_items:\n",
    "            return self\n",
    "\n",
    "        subset = NLIDataset(self.sentences1[from_:to],\n",
    "                            self.sentences2[from_:to],\n",
    "                            self.sizes1[from_:to],\n",
    "                            self.sizes2[from_:to],\n",
    "                            self.labels[from_:to])\n",
    "        return subset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_items\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences1[idx], self.sentences2[idx], self.labels[idx]\n",
    "    \n",
    "\n",
    "class BatchedNLIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, pairs, word_dict, sizes1, sizes2, label_dict):\n",
    "        self.pairs = pairs\n",
    "        self.label_dict = label_dict\n",
    "        self.word_dict = word_dict\n",
    "        self.max_len1 = sizes1\n",
    "        self.max_len2 = sizes2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pairs = self.pairs[idx]\n",
    "        tokens1 = [pair[0] for pair in pairs]\n",
    "        tokens2 = [pair[1] for pair in pairs]\n",
    "        sentences1, sizes1 = utils._convert_pairs_to_indices(tokens1, self.word_dict,\n",
    "                                                             self.max_len1)\n",
    "        sentences2, sizes2 = utils._convert_pairs_to_indices(tokens2, self.word_dict,\n",
    "                                                             self.max_len2)\n",
    "        if self.label_dict is not None:\n",
    "            labels = utils.convert_labels(pairs, self.label_dict)\n",
    "        else:\n",
    "            labels = None\n",
    "        \n",
    "        return sentences1, sentences2, labels\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "class Utils:\n",
    "    def tokenize_english(self, text):\n",
    "        return tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "    def tokenize_corpus(self, pairs):\n",
    "        tokenized_pairs = []\n",
    "        for sent1, sent2, label in pairs:\n",
    "            tokens1 = self.tokenize_english(sent1)\n",
    "            tokens2 = self.tokenize_english(sent2)\n",
    "            tokenized_pairs.append((tokens1, tokens2, label))\n",
    "\n",
    "        return tokenized_pairs\n",
    "\n",
    "    def count_corpus_tokens(self, pairs):\n",
    "        \"\"\"\n",
    "        Examine all pairs ans extracts all tokens from both text and hypothesis.\n",
    "        :param pairs: a list of tuples (sent1, sent2, relation) with tokenized\n",
    "            sentences\n",
    "        :return: a Counter of lowercase tokens\n",
    "        \"\"\"\n",
    "        c = Counter()\n",
    "        for sent1, sent2, _ in pairs:\n",
    "            c.update(t.lower() for t in sent1)\n",
    "            c.update(t.lower() for t in sent2)\n",
    "\n",
    "        return c\n",
    "\n",
    "    def shuffle_arrays(self, *arrays):\n",
    "        rng_state = np.random.get_state()\n",
    "        for array in arrays:\n",
    "            np.random.shuffle(array)\n",
    "            np.random.set_state(rng_state)\n",
    "\n",
    "    def create_label_dict(self, pairs):\n",
    "        \"\"\"\n",
    "        Return a dictionary mapping the labels found in `pairs` to numbers\n",
    "        :param pairs: a list of tuples (_, _, label), with label as a string\n",
    "        :return: a dict\n",
    "        \"\"\"\n",
    "        labels = set(pair[2] for pair in pairs)\n",
    "        mapping = zip(labels, range(len(labels)))\n",
    "        params.nr_classes = len(labels)\n",
    "        return dict(mapping)\n",
    "\n",
    "\n",
    "    def convert_labels(self, pairs, label_map):\n",
    "        \"\"\"\n",
    "        Return a numpy array representing the labels in `pairs`\n",
    "        :param pairs: a list of tuples (_, _, label), with label as a string\n",
    "        :param label_map: dictionary mapping label strings to numbers\n",
    "        :return: a numpy array\n",
    "        \"\"\"\n",
    "        return np.array([label_map[pair[2]] for pair in pairs], dtype=np.int32)\n",
    "\n",
    "\n",
    "    def create_dataset(self, pairs, word_dict, label_dict=None,\n",
    "                       max_len1=None, max_len2=None):\n",
    "        \"\"\"\n",
    "        Generate and return a RTEDataset object for storing the data in numpy format.\n",
    "        :param pairs: list of tokenized tuples (sent1, sent2, label)\n",
    "        :param word_dict: a dictionary mapping words to indices\n",
    "        :param label_dict: a dictionary mapping labels to numbers. If None,\n",
    "            labels are ignored.\n",
    "        :param max_len1: the maximum length that arrays for sentence 1\n",
    "            should have (i.e., time steps for an LSTM). If None, it\n",
    "            is computed from the data.\n",
    "        :param max_len2: same as max_len1 for sentence 2\n",
    "        :return: RTEDataset\n",
    "        \"\"\"\n",
    "        tokens1 = [pair[0] for pair in pairs]\n",
    "        tokens2 = [pair[1] for pair in pairs]\n",
    "        sentences1, sizes1 = self._convert_pairs_to_indices(tokens1, word_dict,\n",
    "                                                            max_len1)\n",
    "        sentences2, sizes2 = self._convert_pairs_to_indices(tokens2, word_dict,\n",
    "                                                            max_len2)\n",
    "        if label_dict is not None:\n",
    "            labels = self.convert_labels(pairs, label_dict)\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        return NLIDataset(sentences1, sentences2, sizes1, sizes2, labels)\n",
    "    \n",
    "    \n",
    "    def collate_batch(self, batch):\n",
    "        if params.use_optimizations:\n",
    "            premise = np.array([k for bat in batch for k in bat[0]], dtype=np.float)\n",
    "            hypo = np.array([k for bat in batch for k in bat[1]], dtype=np.float)\n",
    "            labels = np.array([k for bat in batch for k in bat[2]], dtype=np.float)\n",
    "            return torch.from_numpy(premise), torch.from_numpy(hypo), torch.from_numpy(labels)\n",
    "        else:\n",
    "            return default_collate(batch)\n",
    "\n",
    "\n",
    "    def _convert_pairs_to_indices(self, sentences, word_dict, max_len=None,\n",
    "                                  use_null=True):\n",
    "        sizes = np.array([len(sent) for sent in sentences])\n",
    "        if use_null:\n",
    "            sizes += 1\n",
    "            if max_len is not None:\n",
    "                max_len += 1\n",
    "\n",
    "        if max_len is None:\n",
    "            max_len = sizes.max()\n",
    "\n",
    "        shape = (len(sentences), max_len)\n",
    "        array = np.full(shape, word_dict[PADDING], dtype=np.int32)\n",
    "\n",
    "        for i, sent in enumerate(sentences):\n",
    "            words = []\n",
    "            \n",
    "            if len(sent) <= max_len - 1:\n",
    "                words = sent\n",
    "            else:\n",
    "                idx = 0\n",
    "                while len(words) < max_len - 1:\n",
    "                    words.append(sent[idx])\n",
    "                    idx += 1\n",
    "                \n",
    "            indices = [word_dict[token] for token in words]\n",
    "\n",
    "            if use_null:\n",
    "                indices = [word_dict[GO]] + indices\n",
    "\n",
    "            array[i, :len(indices)] = indices\n",
    "\n",
    "        return array, sizes\n",
    "\n",
    "\n",
    "    def load_parameters(self, dirname):\n",
    "        filename = os.path.join(dirname, 'model-params.json')\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def get_sentence_sizes(self, pairs):\n",
    "        sizes1 = np.array([len(pair[0]) for pair in pairs])\n",
    "        sizes2 = np.array([len(pair[1]) for pair in pairs])\n",
    "        return (sizes1, sizes2)\n",
    "\n",
    "\n",
    "    def get_max_sentence_sizes(self, pairs1, pairs2):\n",
    "        train_sizes1, train_sizes2 = self.get_sentence_sizes(pairs1)\n",
    "        valid_sizes1, valid_sizes2 = self.get_sentence_sizes(pairs2)\n",
    "        train_max1 = max(train_sizes1)\n",
    "        valid_max1 = max(valid_sizes1)\n",
    "        max_size1 = max(train_max1, valid_max1)\n",
    "        train_max2 = max(train_sizes2)\n",
    "        valid_max2 = max(valid_sizes2)\n",
    "        max_size2 = max(train_max2, valid_max2)\n",
    "\n",
    "        return max_size1, max_size2\n",
    "\n",
    "\n",
    "    def normalize_embeddings(self, embeddings):\n",
    "        \"\"\"\n",
    "        Normalize the embeddings to have norm 1.\n",
    "        :param embeddings: 2-d numpy array\n",
    "        :return: normalized embeddings\n",
    "        \"\"\"\n",
    "        # normalize embeddings\n",
    "        norms = np.linalg.norm(embeddings.numpy(), axis=1).reshape((-1, 1))\n",
    "        embeddings = torch.from_numpy(embeddings.numpy() / norms)\n",
    "        return embeddings\n",
    "    \n",
    "    def save_checkpoint(self, state, is_best, filename='checkpoint.pth.tar'):\n",
    "        torch.save(state, filename)\n",
    "        epoch = state['epoch']\n",
    "        print(\"=> Saving model to %s\" % filename)\n",
    "        if epoch % 50 == 0 and epoch != 0:\n",
    "            shutil.copyfile(filename, 'model_' + str(epoch) + '.pth.tar')\n",
    "        if is_best:\n",
    "            print(\"=> The model just saved has performed best on validation set\" +\n",
    "                     \" till now\")\n",
    "            shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "        \n",
    "        return filename\n",
    "\n",
    "\n",
    "    def load_checkpoint(self, resume):\n",
    "        if os.path.isfile(resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "            checkpoint = torch.load(resume)\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                     .format(resume, checkpoint['epoch']))\n",
    "            return checkpoint\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "            return None\n",
    "    \n",
    "    def get_time_hhmmss(self, start = None):\n",
    "        \"\"\"\n",
    "        Calculates time since `start` and formats as a string.\n",
    "        \"\"\"\n",
    "        if start is None:\n",
    "            return time.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "        end = time.time()\n",
    "        m, s = divmod(end - start, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        time_str = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "        return time_str  \n",
    "\n",
    "utils = Utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IO:\n",
    "    def _generate_random_vector(self, size):\n",
    "        \"\"\"\n",
    "        Generate a random vector from a uniform distribution between\n",
    "        -0.1 and 0.1.\n",
    "        \"\"\"\n",
    "        return np.random.uniform(-0.1, 0.1, size)\n",
    "\n",
    "    def write_extra_embeddings(self, embeddings, dirname):\n",
    "        \"\"\"\n",
    "        Write the extra embeddings (for unknown, padding and null)\n",
    "        to a numpy file. They are assumed to be the first three in\n",
    "        the embeddings model.\n",
    "        \"\"\"\n",
    "        path = os.path.join(dirname, 'extra-embeddings.npy')\n",
    "        torch.save(embeddings[:3], path)\n",
    "        \n",
    "    def load_embeddings(self, normalize=True, generate=True):\n",
    "        glove = vocab.GloVe(name='6B', dim=params.projection_dim)\n",
    "        wordlist, embeddings = glove.stoi, glove.vectors\n",
    "\n",
    "        mapping = zip(wordlist, range(3, len(wordlist) + 3))\n",
    "\n",
    "        # always map OOV words to 0\n",
    "        wd = defaultdict(int, mapping)\n",
    "        wd[UNKNOWN] = 0\n",
    "        wd[PADDING] = 1\n",
    "        wd[GO] = 2\n",
    "\n",
    "        if generate:\n",
    "            vector_size = embeddings.shape[1]\n",
    "            extra = torch.FloatTensor([\n",
    "                     self._generate_random_vector(vector_size),\n",
    "                     self._generate_random_vector(vector_size),\n",
    "                     self._generate_random_vector(vector_size)])\n",
    "            self.write_extra_embeddings(extra, \"/scratch/as10656/\")\n",
    "\n",
    "        else:\n",
    "            path = os.path.join(load_extra_from, 'extra-embeddings.npy')\n",
    "            extra = torch.load(path)\n",
    "\n",
    "        embeddings = torch.cat((extra, embeddings), 0)\n",
    "\n",
    "\n",
    "        print('Embeddings have shape {}'.format(embeddings.shape))\n",
    "        if normalize:\n",
    "            embeddings = utils.normalize_embeddings(embeddings)\n",
    "\n",
    "        nn_embedding = nn.Embedding(embeddings.shape[0],\n",
    "                                    embeddings.shape[1])\n",
    "\n",
    "        nn_embedding.weight.data.copy_(embeddings)\n",
    "\n",
    "        # Fix weights for training\n",
    "        nn_embedding.weight.requires_grad = False\n",
    "\n",
    "\n",
    "        return wd, nn_embedding\n",
    "\n",
    "    def read_corpus(self, filename, lowercase, language='en'):\n",
    "        print('Reading data from %s' % filename)\n",
    "        # we are only interested in the actual sentences + gold label\n",
    "        # the corpus files has a few more things\n",
    "        useful_data = []\n",
    "        # the Multinli corpus has one JSON object per line\n",
    "        with open(filename, 'rb') as f:\n",
    "            for line in f:\n",
    "                line = line.decode('utf-8')\n",
    "                if lowercase:\n",
    "                    line = line.lower()\n",
    "                data = json.loads(line)\n",
    "                if data['gold_label'] == '-':\n",
    "                    # ignore items without a gold label\n",
    "                    continue\n",
    "                    \n",
    "                sentence1_parse = data['sentence1_parse']\n",
    "                sentence2_parse = data['sentence2_parse']\n",
    "                label = data['gold_label']\n",
    "\n",
    "                tree1 = nltk.Tree.fromstring(sentence1_parse)\n",
    "                tree2 = nltk.Tree.fromstring(sentence2_parse)\n",
    "                tokens1 = tree1.leaves()\n",
    "                tokens2 = tree2.leaves()\n",
    "                t = (tokens1, tokens2, label)\n",
    "                useful_data.append(t)\n",
    "\n",
    "        return useful_data\n",
    "\n",
    "    def read_corpus_batched(self, filename, lowercase, language='en'):\n",
    "        print('Reading data from %s' % filename)\n",
    "        # we are only interested in the actual sentences + gold label\n",
    "        # the corpus files has a few more things\n",
    "        useful_data = []\n",
    "        done = dict()\n",
    "        # the Multinli corpus has one JSON object per line\n",
    "        with open(filename, 'rb') as f:\n",
    "            for line in f:\n",
    "                line = line.decode('utf-8')\n",
    "                if lowercase:\n",
    "                    line = line.lower()\n",
    "                data = json.loads(line)\n",
    "                if data['gold_label'] == '-':\n",
    "                    # ignore items without a gold label\n",
    "                    continue\n",
    "                prompt_id = data['promptid']\n",
    "                \n",
    "                if prompt_id not in done:\n",
    "                    done[prompt_id] = len(useful_data)\n",
    "                    useful_data.append([])\n",
    "                \n",
    "                sentence1_parse = data['sentence1_parse']\n",
    "                sentence2_parse = data['sentence2_parse']\n",
    "                label = data['gold_label']\n",
    "\n",
    "                tree1 = nltk.Tree.fromstring(sentence1_parse)\n",
    "                tree2 = nltk.Tree.fromstring(sentence2_parse)\n",
    "                tokens1 = tree1.leaves()\n",
    "                tokens2 = tree2.leaves()\n",
    "                t = (tokens1, tokens2, label)\n",
    "                useful_data[done[prompt_id]].append(t)\n",
    "\n",
    "        return useful_data\n",
    "    \n",
    "io = IO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    \"\"\"\n",
    "    Provides early stopping functionality. Keeps track of model accuracy, \n",
    "    and if it doesn't improve over time restores last best performing \n",
    "    parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, patience = 100, minimize = True):\n",
    "        \"\"\"\n",
    "        Initialises a `EarlyStopping` isntance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        saver     : \n",
    "                    TensorFlow Saver object to be used for saving and restoring model.\n",
    "        session   : \n",
    "                    TensorFlow Session object containing graph where model is restored.\n",
    "        patience  : \n",
    "                    Early stopping patience. This is the number of epochs we wait for \n",
    "                    accuracy to start improving again before stopping and restoring \n",
    "                    previous best performing parameters.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        New instance.\n",
    "        \"\"\"\n",
    "        self.minimize = minimize\n",
    "        self.patience = patience\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.best_monitored_value = 0. if minimize else np.inf\n",
    "        self.best_monitored_acc = np.inf if minimize else 0.\n",
    "        self.best_monitored_epoch = 0\n",
    "\n",
    "        self.restore_path = None\n",
    "\n",
    "    def __call__(self, value, acc, epoch, rest):\n",
    "        \"\"\"\n",
    "        Checks if we need to stop and restores the last well performing values if we do.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        value     : \n",
    "                    Last epoch monitored value.\n",
    "        acc       :\n",
    "                    Current accuracy\n",
    "        epoch     : \n",
    "                    Last epoch number.\n",
    "                  \n",
    "        Returns\n",
    "        -------\n",
    "        `True` if we waited enough and it's time to stop and we restored the \n",
    "        best performing weights, or `False` otherwise.\n",
    "        \"\"\"\n",
    "        if (self.minimize and acc < self.best_monitored_acc) or (not self.minimize and acc > self.best_monitored_acc):\n",
    "            self.best_monitored_value = value\n",
    "            self.best_monitored_epoch = epoch\n",
    "            self.best_monitored_acc = acc\n",
    "            state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': self.model.state_dict(),\n",
    "                'best': value,\n",
    "                'best_acc': acc,\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "            }            \n",
    "            \n",
    "            rest.update(state)\n",
    "            self.restore_path = utils.save_checkpoint(rest, True, \"/scratch/as10656/nli_models/early_stopping_checkpoint\")\n",
    "        elif self.best_monitored_epoch + self.patience < epoch:\n",
    "            if self.restore_path != None:\n",
    "                checkpoint = utils.load_checkpoint(self.restore_path)\n",
    "                self.best_monitored_value = checkpoint['best']\n",
    "                self.best_monitored_acc = checkpoint['best_acc']\n",
    "                self.best_monitored_epoch = checkpoint['epoch']\n",
    "                self.model.load_state_dict(checkpoint['state_dict'])\n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            else:\n",
    "                print(\"ERROR: Failed to restore session\")\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def init_from_checkpoint(self, checkpoint):\n",
    "        self.best_monitored_value = checkpoint['best']\n",
    "        self.best_monitored_acc = checkpoint['best_acc']\n",
    "        self.best_monitored_epoch = 0\n",
    "    \n",
    "    def print_info(self):\n",
    "        print(\"Best loss: {0}, Best Accuracy: {1}, at epoch {2}\"\n",
    "              .format(self.best_monitored_value,\n",
    "                      self.best_monitored_acc,\n",
    "                      self.best_monitored_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_curve(axis, params, train_column, valid_column, linewidth = 2, train_linestyle = \"b-\", valid_linestyle = \"g-\"):\n",
    "    \"\"\"\n",
    "    Plots a pair of validation and training curves on a single plot.\n",
    "    \"\"\"\n",
    "    train_values = params[train_column]\n",
    "    valid_values = params[valid_column]\n",
    "    epochs = train_values.shape[0]\n",
    "    x_axis = np.arange(epochs)\n",
    "    axis.plot(x_axis[train_values > 0], train_values[train_values > 0], train_linestyle, linewidth=linewidth, label=\"train\")\n",
    "    axis.plot(x_axis[valid_values > 0], valid_values[valid_values > 0], valid_linestyle, linewidth=linewidth, label=\"valid\")\n",
    "    return epochs\n",
    "\n",
    "# Plots history of learning curves for a specific model.\n",
    "def plot_learning_curves(params):\n",
    "    \"\"\"\n",
    "    Plots learning curves (loss and accuracy on both training and validation sets) for a model identified by a parameters struct.\n",
    "    \"\"\"\n",
    "    curves_figure = plt.figure(figsize = (10, 4))\n",
    "    axis = curves_figure.add_subplot(1, 2, 1)\n",
    "    epochs_plotted = plot_curve(axis, params, train_column = \"train_acc\", valid_column = \"val_acc\")\n",
    "\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.ylim(45., 115.)\n",
    "    plt.xlim(0, epochs_plotted)\n",
    "\n",
    "    axis = curves_figure.add_subplot(1, 2, 2)\n",
    "    epochs_plotted = plot_curve(axis, params, train_column = \"train_loss\", valid_column = \"val_loss\")\n",
    "\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.ylim(1, 50)\n",
    "    plt.xlim(0, epochs_plotted)\n",
    "#     plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have shape torch.Size([400003, 200])\n"
     ]
    }
   ],
   "source": [
    "wd, embeddings = io.load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from /home/as10656/nli-batch-optimizations/multinli_1.0/multinli_1.0_dev_matched.jsonl\n",
      "Reading data from /home/as10656/nli-batch-optimizations/multinli_1.0/multinli_1.0_train.jsonl\n"
     ]
    }
   ],
   "source": [
    "dev_pairs = io.read_corpus(params.dev_file, True)\n",
    "label_dict = utils.create_label_dict(dev_pairs)\n",
    "dev_data = utils.create_dataset(dev_pairs, wd, label_dict, max_len1=params.max_len, max_len2=params.max_len)\n",
    "\n",
    "if params.use_optimizations:\n",
    "    train_pairs = io.read_corpus_batched(params.train_file, True)\n",
    "#     train_pairs = [pair for pairs in train_pairs for pair in pairs]\n",
    "    train_data = BatchedNLIDataset(train_pairs, wd, params.max_len, params.max_len, label_dict)\n",
    "else:\n",
    "    train_pairs = io.read_corpus(params.train_file, True)\n",
    "    train_data = utils.create_dataset(train_pairs, wd, label_dict, max_len1=params.max_len, max_len2=params.max_len)\n",
    "\n",
    "\n",
    "del train_pairs\n",
    "del dev_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Taken from allennlp\n",
    "class TimeDistributed(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Given an input shaped like ``(batch_size, time_steps, [rest])``\n",
    "    and a ``Module`` that takes inputs like ``(batch_size, [rest])``,\n",
    "    ``TimeDistributed`` reshapes the input to be\n",
    "    ``(batch_size * time_steps, [rest])``, applies the contained ``Module``,\n",
    "    then reshapes it back.\n",
    "    Note that while the above gives shapes with ``batch_size`` first,\n",
    "    this ``Module`` also works if ``batch_size`` is second -\n",
    "    we always just combine the first two dimensions, then split them.\n",
    "    \"\"\"\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self._module = module\n",
    "\n",
    "    def forward(self, *inputs):  # pylint: disable=arguments-differ\n",
    "        reshaped_inputs = []\n",
    "        for input_tensor in inputs:\n",
    "            input_size = input_tensor.size()\n",
    "            if len(input_size) <= 2:\n",
    "                raise RuntimeError(\"No dimension to distribute: \" +\n",
    "                                   str(input_size))\n",
    "\n",
    "            # Squash batch_size and time_steps into a single axis;\n",
    "            # result has shape (batch_size * time_steps, input_size).\n",
    "            squashed_shape = [-1] + [x for x in input_size[2:]]\n",
    "            reshaped_inputs.append(\n",
    "                input_tensor.contiguous().view(*squashed_shape))\n",
    "\n",
    "        reshaped_outputs = self._module(*reshaped_inputs)\n",
    "\n",
    "        # Now get the output back into the right shape.\n",
    "        # (batch_size, time_steps, [hidden_size])\n",
    "        new_shape = [input_size[0], input_size[1]] + \\\n",
    "                    [x for x in reshaped_outputs.size()[1:]]\n",
    "        outputs = reshaped_outputs.contiguous().view(*new_shape)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(model):\n",
    "    # As mentioned in paper\n",
    "    mean = 0\n",
    "    stddev = 0.01\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean, stddev)\n",
    "\n",
    "\n",
    "def get_embedded_mask(embedded):\n",
    "    return (embedded != 1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from allennlp.modules import MatrixAttention, SimilarityFunction\n",
    "# from allennlp.nn.util import get_text_field_mask, last_dim_softmax, weighted_sum\n",
    "\n",
    "class DecomposableAttention(nn.Module):\n",
    "    def __init__(self, settings, embeddings):\n",
    "        super(DecomposableAttention, self).__init__()\n",
    "\n",
    "        if isinstance(settings, argparse.Namespace):\n",
    "            settings = vars(settings)\n",
    "            \n",
    "        self.embedding = embeddings\n",
    "        self.settings = settings\n",
    "        self.max_length = settings['max_len']\n",
    "        self.hidden_dim = settings['hidden_dim']\n",
    "        self.nr_classes = settings['nr_classes']\n",
    "        \n",
    "        if settings['hidden_dim'] != settings['projection_dim']:\n",
    "            self.projection = nn.Linear(self.hidden_dim, settings['prjection_dim'])\n",
    "        else:\n",
    "            self.projection = lambda x: x\n",
    "            \n",
    "        settings['hidden_dim'] = settings['projection_dim']\n",
    "        self.hidden_dim = settings['hidden_dim']\n",
    "\n",
    "        if settings['gru_encode']:\n",
    "            self.encoder = BiRNNEncoder(self.max_length, self.hidden_dim,\n",
    "                                        dropout=settings['dropout'])\n",
    "        if settings['use_intra_attention']:\n",
    "            self.intra_sentence_attender = Attention(self.max_length,\n",
    "                                                     self.hidden_dim,\n",
    "                                                     dropout=settings['dropout'])\n",
    "\n",
    "    #         similarity_function = SimilarityFunction.from_params({'type': 'dot_product'})\n",
    "    #         self.matrix_attention = MatrixAttention(similarity_function)\n",
    "\n",
    "            self.intra_align = SoftAlignment(self.max_length, self.hidden_dim)\n",
    "            self.intra_align_project = nn.Linear(self.hidden_dim * 2,\n",
    "                                                 self.hidden_dim)\n",
    "\n",
    "        self.attender = Attention(self.max_length, self.hidden_dim,\n",
    "                                  dropout=settings['dropout'])\n",
    "\n",
    "        self.align = SoftAlignment(self.max_length, self.hidden_dim)\n",
    "\n",
    "        self.compare = Comparison(self.max_length, self.hidden_dim,\n",
    "                                  dropout=settings['dropout'])\n",
    "        self.aggregate = Aggregate(self.hidden_dim, self.nr_classes,\n",
    "                                   dropout=settings['dropout'])\n",
    "\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, premise, hypo):\n",
    "        premise_mask = get_embedded_mask(premise)\n",
    "        hypo_mask = get_embedded_mask(hypo)\n",
    "        \n",
    "        premise = self.embedding(premise)\n",
    "        hypo = self.embedding(hypo)\n",
    "\n",
    "        premise = self.projection(premise)\n",
    "        hypo = self.projection(hypo)\n",
    "\n",
    "\n",
    "        if self.settings['gru_encode']:\n",
    "            premise = self.encoder(premise)\n",
    "            hypo = self.encoder(hypo)\n",
    "\n",
    "        if self.settings['use_intra_attention']:\n",
    "            # Intra Sentence Attention\n",
    "            premise = self.intra_attention(premise, premise_mask)\n",
    "            hypo = self.intra_attention(hypo, hypo_mask)\n",
    "\n",
    "            premise = self.intra_align_project(premise)\n",
    "            hypo = self.intra_align_project(hypo)\n",
    "\n",
    "        projected_premise = self.attender(premise)\n",
    "        projected_hypo = self.attender(hypo)\n",
    "\n",
    "        att_ji = projected_hypo.bmm(projected_premise.permute(0, 2, 1))\n",
    "\n",
    "        # Shape: batch_length * max_length * max_length\n",
    "        att_ij = att_ji.permute(0, 2, 1)\n",
    "\n",
    "        aligned_hypo = self.align(hypo, hypo_mask, att_ij)\n",
    "        aligned_premise = self.align(premise, premise_mask, att_ij, transpose=True)\n",
    "        \n",
    "        \n",
    "#         similarity_matrix = self._matrix_attention(projected_premise, projected_hypo)\n",
    "\n",
    "#         # Shape: (batch_size, premise_length, hypothesis_length)\n",
    "#         p2h_attention = last_dim_softmax(similarity_matrix, hypo_mask)\n",
    "#         # Shape: (batch_size, premise_length, embedding_dim)\n",
    "#         aligned_hypothesis = weighted_sum(embedded_hypo, p2h_attention)\n",
    "\n",
    "#         # Shape: (batch_size, hypothesis_length, premise_length)\n",
    "#         h2p_attention = last_dim_softmax(similarity_matrix.transpose(1, 2).contiguous(), premise_mask)\n",
    "#         # Shape: (batch_size, hypothesis_length, embedding_dim)\n",
    "#         aligned_premise = weighted_sum(premise, h2p_attention)\n",
    "\n",
    "        premise_compare_input = torch.cat([premise, aligned_hypo], dim=-1)\n",
    "        hypo_compare_input = torch.cat([hypo, aligned_premise], dim=-1)\n",
    "\n",
    "        compared_premise = self.compare(premise_compare_input)\n",
    "        compared_premise = compared_premise * premise_mask.unsqueeze(-1)\n",
    "        # Shape: (batch_size, compare_dim)\n",
    "        compared_premise = compared_premise.sum(dim=1)\n",
    "\n",
    "        compared_hypo = self.compare(hypo_compare_input)\n",
    "        compared_hypo = compared_hypo * hypo_mask.unsqueeze(-1)\n",
    "        # Shape: (batch_size, compare_dim)\n",
    "        compared_hypo = compared_hypo.sum(dim=1)\n",
    "\n",
    "        aggregate_input = torch.cat([compared_premise, compared_hypo], dim=-1)\n",
    "        scores = self.aggregate(aggregate_input)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def intra_attention(self, sentence, sentence_mask):\n",
    "        projected_sentence = self.intra_sentence_attender(sentence)\n",
    "        intra_att_ji = projected_sentence.bmm(projected_sentence.permute(0, 2,\n",
    "                                                                         1))\n",
    "        intra_att_ij = intra_att_ji.permute(0, 2, 1)\n",
    "        aligned_sentence = self.intra_align(sentence, sentence_mask, intra_att_ij)\n",
    "        sentence = torch.cat([sentence, aligned_sentence], dim=-1)\n",
    "\n",
    "        return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiRNNEncoder(nn.Module):\n",
    "    def __init__(self, max_length, nr_hidden, dropout=0.0):\n",
    "        super(BiRNNEncoder, self).__init__()\n",
    "\n",
    "        self.nr_hidden = nr_hidden\n",
    "\n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.Linear(nr_hidden * 2, nr_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(nr_hidden, nr_hidden, max_length,\n",
    "                            dropout=dropout,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        self.fully_connected = TimeDistributed(self.fully_connected)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        init_weights(self)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output, _ = self.lstm(input)\n",
    "        return self.dropout(self.fully_connected(output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, max_length, nr_hidden, dropout=0.0):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.model = TimeDistributed(nn.Sequential(\n",
    "            nn.Linear(nr_hidden, nr_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(nr_hidden, nr_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout)\n",
    "        ))\n",
    "\n",
    "        init_weights(self.model)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        return self.model(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftAlignment(nn.Module):\n",
    "    def __init__(self, max_length, nr_hidden):\n",
    "        super(SoftAlignment, self).__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.nr_hidden = nr_hidden\n",
    "\n",
    "    def forward(self, sentence, mask, attention_matrix, transpose=False):\n",
    "        if transpose:\n",
    "            attention_matrix = attention_matrix.permute(0, 2, 1)\n",
    "\n",
    "        exponents = torch.exp(attention_matrix -\n",
    "                              torch.max(attention_matrix,\n",
    "                                        dim=-1,\n",
    "                                        keepdim=True)[0])\n",
    "        \n",
    "        exponents = exponents\n",
    "\n",
    "        summation = torch.sum(exponents, dim=-1, keepdim=True)\n",
    "\n",
    "        attention_weights = exponents / summation\n",
    "        \n",
    "        attention_weights = attention_weights * mask.unsqueeze(-1)\n",
    "        \n",
    "        attention_weights = attention_weights / (attention_weights.sum(-1, keepdim=True) + 1e-13) \n",
    "\n",
    "        return attention_weights.bmm(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Comparison(nn.Module):\n",
    "    def __init__(self, max_length, nr_hidden, dropout=0):\n",
    "        super(Comparison, self).__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.nr_hidden = nr_hidden\n",
    "        self.model = TimeDistributed(nn.Sequential(\n",
    "            nn.Linear(nr_hidden * 2, nr_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(nr_hidden, nr_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout)\n",
    "        ))\n",
    "        init_weights(self.model)\n",
    "\n",
    "    def forward(self, concatenated_aligned_sentence):\n",
    "        return self.model(concatenated_aligned_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Aggregate(nn.Module):\n",
    "    def __init__(self, nr_hidden, nr_out, dropout=0.0):\n",
    "        super(Aggregate, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(nr_hidden * 2, nr_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(nr_hidden, nr_out)\n",
    "        )\n",
    "\n",
    "        init_weights(self.model)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, params, train_data, dev_data, embedding):\n",
    "        self.params = params\n",
    "        self.train_data = train_data\n",
    "        self.dev_data = dev_data\n",
    "        self.epochs = params.epochs\n",
    "        print(\"Creating dataloaders\")\n",
    "        self.cuda_available = torch.cuda.is_available()\n",
    "        \n",
    "        self.train_loader = DataLoader(dataset=train_data,\n",
    "                                       shuffle=True,\n",
    "                                       batch_size=params.batch_size,\n",
    "                                       pin_memory=self.cuda_available,\n",
    "                                       collate_fn=utils.collate_batch)\n",
    "        self.dev_loader = DataLoader(dataset=dev_data,\n",
    "                                     shuffle=False,\n",
    "                                     batch_size=params.batch_size,\n",
    "                                     pin_memory=self.cuda_available)\n",
    "        \n",
    "        self.string_fixer = \"==========\"\n",
    "        self.embedding = embedding\n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        model = DecomposableAttention(self.params, embeddings)\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad,\n",
    "                                      model.parameters()),\n",
    "                               lr=params.lr)\n",
    "\n",
    "        start_epoch = 0\n",
    "        best_prec = 0\n",
    "        self.start_time = time.time()\n",
    "        self.histories = {\n",
    "            \"train_loss\": np.empty(0, dtype=np.float32),\n",
    "            \"train_acc\": np.empty(0, dtype=np.float32),\n",
    "            \"val_loss\": np.empty(0, dtype=np.float32),\n",
    "            \"val_acc\": np.empty(0, dtype=np.float32)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        self.early_stopping = EarlyStopping(model, optimizer, patience=self.params.patience, minimize=False)\n",
    "        if self.params.resume:\n",
    "            checkpoint = utils.load_checkpoint(self.params.resume)\n",
    "            if checkpoint is not None:\n",
    "                model.load_state_dict(checkpoint['state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "                self.histories.update(checkpoint)\n",
    "                self.early_stopping.init_from_checkpoint(checkpoint)\n",
    "                print(\"Loaded model, Best Loss: %.8f, Best Acc: %.2f\" % (checkpoint['best'], checkpoint['best_acc']))\n",
    "\n",
    "        is_best = False\n",
    "\n",
    "        if self.cuda_available:\n",
    "            model = model.cuda()\n",
    "\n",
    "        \n",
    "        model.train()\n",
    "        print(\"Starting training\")\n",
    "        self.print_info()\n",
    "        for epoch in range(start_epoch, params.epochs):\n",
    "            for i, (premise, hypo, labels) in enumerate(self.train_loader):\n",
    "                premise_batch = Variable(premise.long())\n",
    "                hypo_batch = Variable(hypo.long())\n",
    "                labels_batch = Variable(labels)\n",
    "                if self.cuda_available:\n",
    "                    premise_batch = premise_batch.cuda()\n",
    "                    hypo_batch = hypo_batch.cuda()\n",
    "                    labels_batch = labels_batch.cuda(async=True)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(premise_batch, hypo_batch)\n",
    "                loss = criterion(output, labels_batch.long())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if self.params.extra_debug and (i + 1) % (self.params.batch_size * 4) == 0:\n",
    "                    print(('Epoch: [{0}/{1}], Step: [{2}/{3}], Loss: {4},')\n",
    "                             .format(epoch + 1,\n",
    "                                     self.params.epochs,\n",
    "                                     i + 1,\n",
    "                                     len(self.train_loader),\n",
    "                                     loss.data[0]))\n",
    "            train_acc, train_loss = self.validate_model(self.train_loader, model)\n",
    "            dev_acc, dev_loss = self.validate_model(self.dev_loader, model)\n",
    "            \n",
    "            self.histories['train_loss'] = np.append(self.histories['train_loss'], [train_loss])\n",
    "            self.histories['val_loss'] = np.append(self.histories['val_loss'], [dev_loss])\n",
    "            self.histories['val_acc'] = np.append(self.histories['val_acc'], [dev_acc])\n",
    "            self.histories['train_acc'] = np.append(self.histories['train_acc'], [train_acc])\n",
    "            \n",
    "            if not self.early_stopping(dev_loss, dev_acc, epoch, self.histories):\n",
    "                self.print_train_info(epoch, train_acc, train_loss, dev_acc, dev_loss)\n",
    "            else:\n",
    "                print(\"Early stopping activated\")\n",
    "                print(\"Restoring earlier state and stopping\")\n",
    "                self.early_stopping.print_info()\n",
    "                plot_learning_curves(self.histories)\n",
    "                plt.show()\n",
    "                break\n",
    "            \n",
    "\n",
    "            \n",
    "    def validate_model(self, loader, model):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        for premise, hypo, labels in loader:\n",
    "            premise_batch = Variable(premise.long(), volatile=True)\n",
    "            hypo_batch = Variable(hypo.long(), volatile=True)\n",
    "            labels_batch = Variable(labels.long())\n",
    "\n",
    "            if self.cuda_available:\n",
    "                premise_batch = premise_batch.cuda()\n",
    "                hypo_batch = hypo_batch.cuda()\n",
    "                labels_batch = labels_batch.cuda()\n",
    "\n",
    "            output = model(premise_batch, hypo_batch)\n",
    "            loss = nn.functional.cross_entropy(output, labels_batch.long(), size_average=False)\n",
    "            total_loss += len(premise_batch) * loss.data\n",
    "            total += len(labels_batch)\n",
    "            \n",
    "            if not self.cuda_available:\n",
    "                correct += (labels_batch == output.max(1)[1]).data.cpu().numpy().sum()\n",
    "            else:\n",
    "                correct += (labels_batch == output.max(1)[1]).data.sum()\n",
    "                \n",
    "        model.train()\n",
    "\n",
    "        average_loss = total_loss[0] / total\n",
    "        return correct / total * 100, average_loss\n",
    "\n",
    "    def print_info(self):\n",
    "        print(self.string_fixer + \" Data \" + self.string_fixer)\n",
    "        print(\"Training set: %d examples\" % (len(self.train_data)))\n",
    "        print(\"Validation set: %d examples\" % (len(self.dev_data)))\n",
    "        print(\"Timestamp: %s\" % utils.get_time_hhmmss())\n",
    "        \n",
    "        print(self.string_fixer + \" Params \" + self.string_fixer)\n",
    "    \n",
    "        print(\"Learning Rate: %f\" % self.params.lr)\n",
    "        print(\"Dropout (p): %f\" % self.params.dropout)\n",
    "        print(\"Batch Size: %d\" % self.params.batch_size)\n",
    "        print(\"Epochs: %d\" % self.params.epochs)\n",
    "        print(\"Patience: %d\" % self.params.patience)\n",
    "        print(\"Resume: %s\" % self.params.resume)\n",
    "        print(\"GRU Encode: %s\" % str(self.params.gru_encode))\n",
    "        print(\"Batch Optimizations: %s\" % str(self.params.use_optimizations))\n",
    "        print(\"Intra Attention: %s\" % str(self.params.use_intra_attention))\n",
    "        \n",
    "    def print_train_info(self, epoch, train_acc, train_loss, dev_acc, dev_loss):\n",
    "        print((self.string_fixer + \" Epoch: {0}/{1} \" + self.string_fixer)\n",
    "              .format(epoch + 1, self.params.epochs))\n",
    "        print(\"Train Loss: %.8f, Train Acc: %.2f\" % (train_loss, train_acc))\n",
    "        print(\"Validation Loss: %.8f, Validation Acc: %.2f\" % (dev_loss, dev_acc))\n",
    "        self.early_stopping.print_info()\n",
    "        print(\"Elapsed Time: %s\" % (utils.get_time_hhmmss(self.start_time)))\n",
    "        print(\"Current timestamp: %s\" % (utils.get_time_hhmmss()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataloaders\n",
      "=> loading checkpoint '/scratch/as10656/nli-intra.model'\n",
      "=> loaded checkpoint '/scratch/as10656/nli-intra.model' (epoch 82)\n",
      "Loaded model, Best Loss: 21.71986755, Best Acc: 71.14\n",
      "Starting training\n",
      "========== Data ==========\n",
      "Training set: 392702 examples\n",
      "Validation set: 9815 examples\n",
      "Timestamp: 2017/12/02 01:34:38\n",
      "========== Params ==========\n",
      "Learning Rate: 0.000100\n",
      "Dropout (p): 0.200000\n",
      "Batch Size: 10\n",
      "Epochs: 300\n",
      "Patience: 30\n",
      "Resume: /scratch/as10656/nli-intra.model\n",
      "GRU Encode: False\n",
      "Batch Optimizations: False\n",
      "Intra Attention: True\n",
      "Early stopping activated\n",
      "Restoring earlier state and stopping\n",
      "Best loss: 21.719867549668873, Best Accuracy: 71.13601630157922, at epoch 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAEKCAYAAABXMPIIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b892aaa0cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# params.resume = '/home/as10656/nli-batch-optimizations/parikh-et-al/model_best.pth.tar'\n",
    "params.resume = '/scratch/as10656/nli-intra.model'\n",
    "# params.resume = False\n",
    "params.use_intra_attention = True\n",
    "params.gru_encode = False\n",
    "params.batch_size = 10\n",
    "params.patience = 30\n",
    "trainer = Trainer(params, train_data, dev_data, embeddings)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
